[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCESR Intern Hub",
    "section": "",
    "text": "Welcome!\nThis website / HTML book is intended to collect resources for Cedar Creek summer interns doing independent research projects, and present those resources in an easily accessible way.\nFor now, the focus is primarily on data management, analysis, and using the R programming language.\nMuch of the content featured is adapted from the work of past CCESR Fellows, including Maggie Anderson, Mariana Cárdenas, and Bea Baselga Cervera.\nThis site is structured in different parts, which can be read in any order you choose, depending on your needs / what you already know. Currently, the first part describes data management practices, the second part goes over data analysis in general, the third part describes R-related software and workflows, and the fourth part is intended to give a primer in R coding.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "dm_intro.html",
    "href": "dm_intro.html",
    "title": "1  The Flow of Data",
    "section": "",
    "text": "In any scientific research, data management is a highly important skill. As ecologists, we should strive to record, organize, document, and back up our data in such a way that it will be accessible and useful to both our future selves and other future researchers.\nIn general, the “flow of data” follows the this general pattern:\n\n“Wild” Data - these are our observations, features of the environment we wish to record as…\nRecorded Data - this is our first record of our observations, with which we ideally will…\n\nBack Up - store copies in an alternative medium / location\nError Check - look over the data for entry errors\nOther Processing - ensure proper filenames, data formats, etc., leading to…\n\nProcessed Data - this is the data that we can use to analyze and answer scientific questions!\n\nThe following chapters will go into detail about this flow, with tips and various things to consider.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Flow of Data</span>"
    ]
  },
  {
    "objectID": "dm_record.html",
    "href": "dm_record.html",
    "title": "2  Recording Data",
    "section": "",
    "text": "2.1 Data Formats\nOne thing to think about when recording your data is the formatting. First, you generally want to have data arranged in a table such that there is one row at the top that acts as a “header”, labeling each column. The rest of the rows should just be values of each of those columns. For example:\nData that is formatted like this is easiest for scientific computing environments to interpret. Data like this can be saved as a simple text file, which allows for easier storage, accessibility, and future-proofing.\nGenerally, you will want to avoid breaking the pattern of a simple table, like repeating header rows, using rows or columns for organization, or merging cells.\nIn data science there is also a commonly used dichotomy is between “wide” and “long” data formats. You are likely more used to wide data, but long data can be more efficient for scientific computing and analysis. Here is a comparison:\nWide Data\nExample table of species abundance (attributes) at four research plots (identifiers):\nLong Data\nExample table of the same data from above:\nAs you can see, long data take more space, but it is easier to select and compare data in a computing environment when it is in this format. You will see this in action in the R programming section of this website!",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Recording Data</span>"
    ]
  },
  {
    "objectID": "dm_record.html#sec-formats",
    "href": "dm_record.html#sec-formats",
    "title": "2  Recording Data",
    "section": "",
    "text": "Variable 1\nVariable 2\nVariable 3\n\n\n\n\n1\nBlue\n45.3\n\n\n2\nBlue\n36.9\n\n\n3\nRed\n39.1\n\n\n4\nGreen\n41.2\n\n\n\n\n\n\n\n\nEasy for humans to read\nFeatures unique rows for an identifier, with columns describing attributes\n\n\n\n\n\nPlot\nSpecies A\nSpecies B\n\n\n\n\nNorth\n3\n3\n\n\nEast\n0\n3\n\n\nSouth\n1\n0\n\n\nWest\n2\n0\n\n\n\n\n\nEasy for computers to read\nFeatures multiple rows for each identifier value, one for each attribute\n\n\n\n\n\nPlot\nSpecies\nCount\n\n\n\n\nNorth\nA\n3\n\n\nNorth\nB\n3\n\n\nEast\nA\n0\n\n\nEast\nB\n3\n\n\nSouth\nA\n1\n\n\nSouth\nB\n0\n\n\nWest\nA\n2\n\n\nWest\nB\n0",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Recording Data</span>"
    ]
  },
  {
    "objectID": "dm_record.html#data-sheet-design",
    "href": "dm_record.html#data-sheet-design",
    "title": "2  Recording Data",
    "section": "2.2 Data Sheet Design",
    "text": "2.2 Data Sheet Design\nWhether you design a data sheet using long or wide format, you will need to use some sort of computer application, either a spreadsheet program, word processor, or a combination of both. You are probably familiar with many of the options, but here are a few -\nSpreadsheet Applications:\n\nMicrosoft Excel - Standalone program, often free with university accounts\nGoogle Sheets - web-based, free to use with Google account, can be integrated directly with R\nLibreOffice Calc - Standalone program, free and open-source alternative to Excel\n\nWord Processors:\n\nMicrosoft Word - Standalone program, often free with university accounts\nGoogle Docs - web-based, free to use with Google account\nLibreOffice Writer - Standalone program, free and open-source alternative to Word\n\nSometimes it can be finicky to design a data sheet directly in a word processor’s table functionality. Instead, you can create the basics of your table in a spreadsheet application, and then copy and paste it into the word processor, and then fine-tune the table to your liking.\nWhen designing your sheet/s, wide data will often be more intuitive and easier for recording, but note that there may be more processing steps required when you get to the analysis stage (and if the data are complex, the data manipulation required can also be complicated). On the other hand, recording data in long format, while somewhat tedious, will be efficient for later processing and analysis steps.\nIn general, wide format for recording data is effective for simple observations when you know exactly how much you are going to record (e.g., a suite of soil attributes for a set of plots). Long format can be effective when the observations you’re recording are more complicated and/or you don’t know how many observations you will be making in a given plot (e.g., recording each plant species with it’s cover class, max height, and disease severity in a set of plots - you might not know how many species you will encounter in each plot!).\nUniversal Tip: Always include a “Notes” column!",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Recording Data</span>"
    ]
  },
  {
    "objectID": "dm_record.html#data-sheet-medium",
    "href": "dm_record.html#data-sheet-medium",
    "title": "2  Recording Data",
    "section": "2.3 Data Sheet Medium",
    "text": "2.3 Data Sheet Medium\nWhen it comes to recording data in the field or in the lab, there are two main styles: using paper data sheets or using electronic spreadsheets/apps. Opinions vary among researchers.\nPaper / Physical Media\n\nTraditional method for ecology research\nResults in having both a hard copy and electronic copy of the data\n\nBackups can include photos/scans of the paper data sheets, as well as back-up electronic files\n\nLess risk of technical glitches\nNeed to consider material; write-in-the-rain and pancil often necessary in field settings\nNeeds a solid organization scheme to make sure paper sheets don’t get lost or misplaced\nRequires data to be entered from paper to computer\n\nBut this does leave the chance for checking for entry errors between paper and computer\n\n\nElectronic Media / App-based Recording\n\nBecoming more popular among large-scale projects (e.g., National Ecological Observatory Network)\nSome projects use propietary software for data recording, but you can use the Google Sheets app\nData is usually only digital files, but can be printed out for hard copy back-ups\nSomewhat elevated risk for technical glitches / user error\nCan utilize immediate cloud backup\nPhone data entry can be particularly convenient for certain field methods\nError checking is limited to looking for obvious field entry errors (e.g., impossible attribute values)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Recording Data</span>"
    ]
  },
  {
    "objectID": "dm_organize.html",
    "href": "dm_organize.html",
    "title": "3  File Organization",
    "section": "",
    "text": "3.1 Organize\nWhen you are developing an organization scheme, there are many ways you can proceed. Here are a few:\nFile Organization Strategies\nThe best way to organize your files will depend on the project and the researcher, but as long as it is consistent, it is effective.\nThe strategies listed above generally work best as coarse-scale organization schemes, but one strategy could be used as a subfolder scheme within another strategy.\nDate and location generally work best as subfolder organization schemes within one of the above strategies. One exception to this might be when you are doing radically different things among years or locations.\nWhen you are downloading research related files, it is best to move them from the “downloads” folder as soon as possible, or better yet, download directly to the appropriate folder and subfolder. And avoiding downloading important files to the desktop is generally helpful.\nFinally, Do not use your folders as the sole description of your files, i.e., use descriptive file names! A file named “plot3” may make sense in your “2023_BioCON” folder, but it will lose context when moved or shared. This brings us to our next section…",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File Organization</span>"
    ]
  },
  {
    "objectID": "dm_organize.html#organize",
    "href": "dm_organize.html#organize",
    "title": "3  File Organization",
    "section": "",
    "text": "By Stage - collection materials, raw data, processed data, shared data, etc.\nBy Data Type - text, images, spreadsheets, etc.\nBy Research Activities - experiments, field observations, interviews, etc.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File Organization</span>"
    ]
  },
  {
    "objectID": "dm_organize.html#name",
    "href": "dm_organize.html#name",
    "title": "3  File Organization",
    "section": "3.2 Name",
    "text": "3.2 Name\nDescriptive file names should be easily scannable and sortable.\nKey components of a descriptive file name often include:\n\nProject name\nDate (YYYYMMDD)\nType of data\nLocation/site\nResearcher Info\nVersion\n\nFor sorting purposes, it is best to use numerical date format and leading zeroes. Numerical date format is YYYYMMDD, or 20240710 instead of something like 7-10-24. When sorted, everything will be in calendar order. Sorting other date formats can lead to files being out of order, like how 12 comes before 7 in an alphanumeric sort, so December could come before July in your files. Similarly, since 10, 11, 12, etc. come before 2 in an alphanumeric sort, it is good to use leading zeroes depending on the number of numeric IDs you have. For example if you have 12 plots, use one leading zero for plots 1-9 to ensure proper file sorting (e.g., “01”, “02”, etc.).\nUniversal Tips:\n\nDon’t use special characters (*@,.^?#!&lt;&gt;)\n\nThese are incompatible with many file systems\n\nBe concise, AKA brief but efficiently informative",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File Organization</span>"
    ]
  },
  {
    "objectID": "dm_document.html",
    "href": "dm_document.html",
    "title": "4  Documentation and Storage",
    "section": "",
    "text": "4.1 Document\nWhat to document:\nWhere to document:\nWhen to document:",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation and Storage</span>"
    ]
  },
  {
    "objectID": "dm_document.html#document",
    "href": "dm_document.html#document",
    "title": "4  Documentation and Storage",
    "section": "",
    "text": "File-specific info or metadata\n\nfile naming patterns\nvariable/attribute definitions\n\nTechnical steps\n\ndata collection\nprocessing (what did you include or exclude?)\nanalysis steps\n\nExternal data sources\nSoftware used with version numbers\nMeeting notes\nOrganization schemes\n\n\n\nLab Notebook\n\nCan be physical or digital\nMany scientists use pen for this sort of thing\n\nField notebook\n\nGenerally easier to be physical if taking notes in the field, can be digital if taking notes at end of day\n\nReadMe file\n\nGood for computer work - data processing and analysis\n\n\n\n\nSet aside ~15 minutes after your work session\nDevelop a regular routine during the research process\n\nEasier to do than remembering everything at the end\n\nCan also note things opportunistically as you go\n\njot potentially relevant observations down in a notebook\ncomment out code",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation and Storage</span>"
    ]
  },
  {
    "objectID": "dm_document.html#store",
    "href": "dm_document.html#store",
    "title": "4  Documentation and Storage",
    "section": "4.2 Store",
    "text": "4.2 Store\nThe methods of backing up your data may vary somewhat based on medium, but what remains important universally is to maintain multiple formats of backups, and it is helpful to have one backup in an alternative physical location.\nPaper Data Sheets\n\ntake a photo or scan each newly filled data sheet after each day of data collection\nBack up entered data files in cloud storage or some other location\n\nElectronic Data\n\nDownload a dated copy of all electronically entered files after each day of data collection\n\nYou can write an R script to do this for you with Google Sheets\n\n\nYou may also run into some issues when storing backups. You may run out of storage space, especially on personal cloud storage accounts. However, usually university or organization based accounts have large storage limits. In addition, physical data drives (either hard drives or solid-state drives) can fail, so it’s good to have cloud-based storage and/or alternate physical drives (another computer or external drive) as fail-safes. Of course, cloud storage providers can change terms in the near future, so be prepared for adapting your plans.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documentation and Storage</span>"
    ]
  },
  {
    "objectID": "dm_process.html",
    "href": "dm_process.html",
    "title": "5  Error Checking and Data Processing",
    "section": "",
    "text": "5.1 Error Checking\nYour error checking method may vary depending on the medium on which your data were recorded, but a general note is that you can elect to error check everything, or a certain percentage of all the data when data are numerous. Different organizations will follow different practices. Oftentimes personal research projects are small scale enough that full error checking is feasible.\nPaper Data\nElectronic Data",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Error Checking and Data Processing</span>"
    ]
  },
  {
    "objectID": "dm_process.html#error-checking",
    "href": "dm_process.html#error-checking",
    "title": "5  Error Checking and Data Processing",
    "section": "",
    "text": "Cross reference digitally entered data with paper sheet for errors in entry\n\nHelpful to have two people\n\nNote any corrections in a new column\nHard to check field recording errors, but make any corrections in a different colored writing utensil for clarity\n\n\n\nCheck version control (e.g., Google Drive files) for unusual edits, and roll back where necessary\n\nNote when you rolled back a change in a new column",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Error Checking and Data Processing</span>"
    ]
  },
  {
    "objectID": "dm_process.html#data-processing",
    "href": "dm_process.html#data-processing",
    "title": "5  Error Checking and Data Processing",
    "section": "5.2 Data Processing",
    "text": "5.2 Data Processing\nWhen you are getting ready to analyze your data, the best file formats are text-based with some sort of delimiter between columns.\nThe most popular of these file formats is the “comma separated value” file, or .csv. It is small in size, as it is a simple text file where commas denote new columns and line breaks denote new rows. This is easy for a wide variety of computer programs to interpret, and will most likely continue to be efficient for the foreseeable future.\nUnlike Excel spreadsheets or Google Sheets, text based format do not support multiple sheets, cell/text colors, or special formatting. As such, do not rely on color or font for describing your electronic data.\nYou can create .csv files form Excel using the “Save as” command and selecting the csv format, and you can download a Google Sheet as a csv as one of the options. You can also “publish” a Google sheet as a csv on the web to import directly into R, but that is a topic for later in this site (see 15.1.3 Importing Data; From the Web)",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Error Checking and Data Processing</span>"
    ]
  },
  {
    "objectID": "da_glance.html",
    "href": "da_glance.html",
    "title": "6  Data Analysis at a Glance",
    "section": "",
    "text": "Analyzing your data is usually about transforming long spreadsheets into a form that is relevant to your question/s, and oftentimes including an appropriate statistical approach for inference.\nYou might use descriptive statistics, which is simply describing what you observed without presenting every data point, and instead a summary of those data. This can often be helpful in providing a frame of reference to your dataset before looking deeper at trends and comparisons. Alternatively, sometimes descriptive statistics are the main goal - like in surveys of populations and communities (e.g., what is the population size of a certain grass of interest in an old field?). Descriptive statistics include things like the mean and variance, but can also include less commonly used measures like dispersion (which is simply the variance : mean ratio!).\nYou could also use inferential statistics, which is more about using math or simulation techniques to infer some conclusion from the shape of your data. This is directly relevant to when you have an ecological question about cause and effect, associations among variables, comparisons among categories, etc. The results of inferential statistics provide a starting point from which to interpret/discuss an answer to your question. Examples include t-tests and linear regression.\nWhen using both of these types of statistics, you should be mindful of data types, which are the form that variables take. For example, the height of a tree is number, but the species of a tree is a category. This contrast is obvious, but there are subtle differences that can be important for how you describe, assess, and plot your data. We will go over data types in the very next section!",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Analysis at a Glance</span>"
    ]
  },
  {
    "objectID": "da_data.html",
    "href": "da_data.html",
    "title": "7  Data Types",
    "section": "",
    "text": "7.1 Numeric Data\nAny data that can be described with numbers or have quantifiable relationships between values is numeric. But! There are multiple types of numeric data. The most important distinction is discrete vs continuous.\nDiscrete numeric data is data where not every value is possible, but you can still quantify specific differences among the possible values - the major example being integer values (1, 2, 3, the rest). Most programming languages will refer to this type as “integer” or “int”. Examples might include number of ants on a log.\nContinuous numeric data is data where every value is possible! So this is basically all real numbers, including decimals (1.0, 1.1, etc.). Many programming languages will refer to this as simply numeric data, but lower level languages might use “float” or “double”. Examples might include the biomass of ants on a log. Note: measures that consist of very large integer values (e.g., when you get up to 4 digits) are approximately continuous.\nOther things to consider with numeric data is whether the scale of measurement is bound by any values. For example, the number of or biomass of ants on a log cannot be less than zero. In addition, percentages and proportions are bound by 0 and 100 and 0 and 1 respectively. These limitations can lead to special considerations when performing inferential statistics.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "da_data.html#categorical-data",
    "href": "da_data.html#categorical-data",
    "title": "7  Data Types",
    "section": "7.2 Categorical Data",
    "text": "7.2 Categorical Data\nAny data for which the values have no specifically quantitative difference among them is categorical. Again there is one majorly important distinction: nominal vs ordinal.\nNominal data is data where categories have no ranking or order, like the species of ants on a log.\nOrdinal data is data where categories have some order, like your top 5 favorite breakfast cereals. But wait! You may be thinking - “isn’t this quantitative?” Well yes and no. The difference between ordinal data and discrete numeric data is that you can’t really quantify the exact difference between ordinal data values. Say there is a go-kart race between Mario, Luigi, and Peach. The place that each finished would be ordinal, e.g., Peach got 1st and Luigi 2nd, but you wouldn’t be able to say how much faster Peach was than Luigi. The time it took for Peach and Luigi each to finish the race would be a numeric variable, and there would be a specific value difference between them.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Types</span>"
    ]
  },
  {
    "objectID": "da_describe.html",
    "href": "da_describe.html",
    "title": "8  Descriptive Statistics",
    "section": "",
    "text": "8.1 Centrality\nYou’ll often want to describe the central tendency of your data - around where are the values centered?\nMean - the average of the values, or the sum of all values divided by the number of observations\nMedian - the value at which half of the observations are greater, and the other half are less\nMode - the most commonly observed value\nUsually, the mean is a perfectly adequate descriptor. You can use it on continuous numeric data, discrete numeric data (though the mean value will often be unrealistic), or even ordinal rankings.\nWhen might you prefer to use the median over the mean?\nWhen the data is skewed such that there are many small values and a few big values, the mean might be inflated by those large values, and thus overestimate the central tendency in some contexts.\nWhen data is roughly normally distributed, the mean and median are roughly the same:\nBut when data are skewed, the median may be a better estimate of the central tendency:",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "da_describe.html#spread",
    "href": "da_describe.html#spread",
    "title": "8  Descriptive Statistics",
    "section": "8.2 Spread",
    "text": "8.2 Spread\nYou also might be interested in how varied your data is, how much it deviates from the central tendency. This can be done with the following:\nVariance - how variable is the data? Measured as the average squared difference between observations and the mean:\n\\[\nVariance = \\frac{\\sum (Observation_i - Mean)^2}{Number of Observations}\n\\]\n(\\(\\sum\\) means “sum of”)\nThe differences are squared to get rid of negative differences, because other wise everything would cancel out and our variance would be zero!\nStandard Deviation - the square root of the variance. This is useful because it is in the same units as the original measurements!",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "da_describe.html#other-descriptors",
    "href": "da_describe.html#other-descriptors",
    "title": "8  Descriptive Statistics",
    "section": "8.3 Other Descriptors",
    "text": "8.3 Other Descriptors\nAnother descriptor that may prove useful is the dispersion, or the variance divided by the mean. This provides an estimate of how skewed the data is - for example, the first plot above has very low dispersion, while the second plot has high dispersion.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "da_describe.html#ecological-community-descriptors",
    "href": "da_describe.html#ecological-community-descriptors",
    "title": "8  Descriptive Statistics",
    "section": "8.4 Ecological Community Descriptors",
    "text": "8.4 Ecological Community Descriptors\nMany of you are interested in describing the species composition of of community. Here’s a few common descriptors:\nSpecies Richness - this is just the number of different species present.\nSpecies Diversity - this is an index that takes into account the richness as well as the relative abundances of each species. E.g. Shannon’s Diversity Index, where higher numbers mean more species more evenly distributed.\nSpecies Evenness - this is an index that estimates specifically how evenly distributed species abundances are. E.g., Pielou’s Evenness, which ranges from 0 to 1, with 1 meaning that each species has equal numbers.\nNote: these measures can apply to any taxonomic distinction, e.g., family richness, order diversity, etc.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "da_infer.html",
    "href": "da_infer.html",
    "title": "9  Inferential Statistics",
    "section": "",
    "text": "9.1 Classic Frequentist Tests\nNow let’s go over some statistical tests! For this section, it can be useful to remind ourselves of the variables involved in a research question:\nIndependent / Explanatory / Predictor Variable: this is either what you are manipulating in an experiment or what your study is designed to capture variation in (e.g., C02 at BioCON, species richness at BigBio).\nDependent / Response Variable: these are what you measure or observe throughout your study, generally hypothesizing that they will differ among the levels of your independent variable (e.g., aboveground biomass in BioCON or BigBio).",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "da_infer.html#classic-frequentist-tests",
    "href": "da_infer.html#classic-frequentist-tests",
    "title": "9  Inferential Statistics",
    "section": "",
    "text": "9.1.1 Assumptions\nWe should mention what these tests generally assume about your data.\nFirst, they assume that your data are independent. This just means that no two observations of your data are more related to each other in a way that isn’t accounted for by a variable. Say you were comparing mean tree height between two forests - individual tree heights in the same forest would be independent, but two measures of the same tree on different days would be non-independent.\nSecond, they assume that the errors are normally distributed. This is a bit more confusing without a statistical background. An example may be illustrative - in the tree height example above, we assume that the individual tree heights are normally distributed around the mean. Without getting too much into the weeds, if you collect enough data (i.e., 30+ observations), these errors will likely be approximately normally distributed. However, things get dicey when we deal with data that is not continuous like tree height, for example, discrete count data - more on that below.\nThird, they assume homogeneity of variance. This is another complicated one, but it mean that the variance of the errors doesn’t change with the independent variable. In the tree example, we are assuming that the variance of the differences between observed tree heights and the forest mean does not change between forests.\nData that break the first assumption are difficult to deal with outside of accounting for the non-independence factor (which can severely reduce the size of your sample), but failing to meet the second or third assumptions generally leads to transforming data or using alternative tests.\n\n\n9.1.2 Categorical Predictor/s, Numeric Response\n\n9.1.2.1 Two Predictor Categories\nWhen you are comparing numeric values from two groups, you can use a t-test to compare their means. T-tests can be paired when each observation in one group is specifically linked to an observation in the other group (e.g., masses of sibling plants in separate treatments) which can be more powerful. When the variance of values in each group is different, you can do a t-test with unequal variance.\nThe effect size here is the difference between means.\n\n\n9.1.2.2 More Than Two Predictor Categories\nIf you have more than two groups/categories, you can use a Analysis of Variance or ANOVA. This will tell you if the means of each group are equivalent, or if there is at least one inequality. You can test for pairwise comparisons among the groups with Tukey’s test. If you have multiple categorical predictors, you can do two-way or three-way ANOVAs. Tests with more than three categorical predictor variables are uncommon and harder to interpret.\nThe effect sizes are the pairwise difference in means.\n\n\n9.1.2.3 Ordinal Predictors\nWhen your predictor variable is ordinal, the quick and easy way to analyze it would be to convert the predictor to a numeric integer data type and proceed from there. However this is imprecise…\nThis section is under construction\n\n\n\n9.1.3 Numeric Predictor/s, Numeric Response\n\n9.1.3.1 Simple Association\nWhen all you are interested in is whether two numeric variables are related to one another, not cause and effect, you can do a correlation test. Pearson’s correlation is generally applicable for continuous data. Spearman’s correlation is good for when you are dealing with data with non-normal distributions, like count data (it also works for ordinal data!).\nThe effect size here will be a correlation coefficient ranging from -1 to 1, with -1 means an inverse relationship, 0 means no relationship, and 1 mean a direct positive relationship.\n\n\n9.1.3.2 Cause and Effect\nWhen you are suppose a causal relationship between numeric variables, you can use a linear regression. This will use linear algebra or maximum likelihood estimation (don’t worry about the finer details here) to find the best fit line that describes the relationship between two variables; where the sum of the squared distances from the observations to the line is minimized. You can also include multiple predictor variables to perform multiple linear regression AKA multivariate linear regression.\nWhen your response variable is count data, the assumptions of simple linear regression are usually unmet, so you can use generalized forms like a Poisson regression or a Negative Binomial regression.\nThe effect sizes here are the parameter coefficients, i.e., how much does the response change for on unit increase in the predictor? Note: these are not straightforward for Poisson and negative binomial regression, so ask your mentor.\n\n\n\n9.1.4 Numeric Predictor/s, Categorical Response\n\n9.1.4.1 Binary Response\nWhen your categorical response is only two categories (e.g., presence or absence), you can use a binomial regression AKA logistic regression. This works similarly to linear regression, but the effect sizes are measured in log odds, which is difficult to interpret, but can be transformed to estimating how the probability of observing one category value instead of the other increases with a variable.\n\n\n9.1.4.2 Multiple Response Categories\nMultinomial regression (under construction)\n\n\n\n9.1.5 Categorical Predictor/s, Categorical Response\nChi-square test (under construction)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "da_infer.html#bootstrapping",
    "href": "da_infer.html#bootstrapping",
    "title": "9  Inferential Statistics",
    "section": "9.2 Bootstrapping",
    "text": "9.2 Bootstrapping\nOne alternative to these classic tests has no assumptions: bootstrapping. Essentially, it involves using the sampled data to simulate more samples, and compare your observations to those simulations.\nEmpirical Bootstrapping is where you take your actual observations and shuffle which value is associated with which observation. For example, you could take measurements of tree heights from two forests, and randomly assign forest ID to each measurement.\nParametric Bootstrapping is where you summarize your observed data and use it to generate simulated data. For example, you could calculate the mean and variance of tree heights in two forests and then generate simulated forests of trees through random pulls from a normal distribution with the appropriate mean and variance.\nWith both approaches, you simulate a large number of simulated datasets (1000+), and then calculate whatever you are interested in for each of those simulations, and compare the calculation from the observed data to the distribution of simulated values. For example, if you empirically bootstrap the two forests of tree heights 1000 times, and then calculate difference in means for each you will have 1000 mean difference values. The proportion of those simulated values that are equal to or more extreme than your observed mean difference is your p-value!",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "rc_r.html",
    "href": "rc_r.html",
    "title": "10  R Itself",
    "section": "",
    "text": "10.1 R, the Language\nR is a programming language designed for statistical computing, and is often the language of choice for scientists. R is also used for data science in some business, tech, and health contexts (but many prefer Python in those areas).\nAs a programming language it is essentially an expandable collection of functions with syntax to perform tasks, and it could be written in any text editor. However, in order for your computer to interpret the language, it needs some software.",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Itself</span>"
    ]
  },
  {
    "objectID": "rc_r.html#r-the-software",
    "href": "rc_r.html#r-the-software",
    "title": "10  R Itself",
    "section": "10.2 R, the Software",
    "text": "10.2 R, the Software\nThe R application allows you to run R code on your computer, and comes with a basic “console” window where code is run and output is printed, as well as a basic script editor where you can write code to run.\nYou can download the application from here:\nhttps://cran.r-project.org/\nIf you are asked to select a mirror, simply select the nearest one (I believe Iowa State should work).\nIf you have a Windows machine, it should be fairly straightforward to simply download and install the “base” R from the link.\nIf you have a Mac, you will want to select the .pkg file that matches your processor type: x-86 for Intel processors (mostly Macs pre-2020), arm64 for Macs with the M1 or M2 chip (most Macs post-2020).\nIf you are using Linux, you know more than me.",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Itself</span>"
    ]
  },
  {
    "objectID": "rc_r.html#r-packages",
    "href": "rc_r.html#r-packages",
    "title": "10  R Itself",
    "section": "10.3 R Packages",
    "text": "10.3 R Packages\nAs mentioned above, R is expandable. You can add more functionality to R by installing packages. Packages contain more options of code to use to process and analyze data, and also do many other things.\nPackages can be installed through writing R code, or by clicking some buttons in RStudio. Then they will live in a directory that was built when you installed R for auxiliary packages.\nWe will discuss more about installing packages in the R coding section.",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>R Itself</span>"
    ]
  },
  {
    "objectID": "rc_rstudio.html",
    "href": "rc_rstudio.html",
    "title": "11  R Studio",
    "section": "",
    "text": "11.1 RStudio at a Glance\nIf you open up RStudio, you will see something like this:\n1- Script Editor: Here is where you will write code! You can create an R script (a text document to save code in) with the file tab, and write what you need in the resulting script. It is highly recommended to use scripts, because then you can save your code for later, and troubleshoot errors easier. From this window, you can highlight code and run it with the “Run” button on top, or with Ctrl + Enter / Cmd +Enter.\n2- R Console: Here is where the action happens - code will run here, and text output, warnings and messages will be displayed. You can also type code into the console, but that is only recommended for installing packages, entering credentials, rendering documents, and things of that nature. Don’t type your data processing or analysis code into the console, use a script instead! There’s also a terminal tab if you ever need to perform shell commands (which is probably unlikely for your project this summer).\n3- Environment and History: Here you can find a list of the variables and data you have loaded into your “workspace” or “environment” in the Environment tab. These are objects you can do stuff with with code. You can also click the History tab to see the code you have run thus far.\n4- Files and Plots: Here is where any figures you draw will pop up (and you can save them from here as well). There is also a Files tab that allows you to navigate through your file directory (helpful with projects, described below). The Packages tab shows which packages you have installed and loaded (you can also click “Install” at top to easily install new ones!). Finally, the help tab is where you can search for the documentation on any R function.",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "rc_rstudio.html#sec-rprojects",
    "href": "rc_rstudio.html#sec-rprojects",
    "title": "11  R Studio",
    "section": "11.2 R Projects",
    "text": "11.2 R Projects\nIt is highly recommended to use R Projects when working with RStudio. Projects are essentially just subdirectories in your file folders, but they come with a special .Rproj file that RStudio can read and use. This helps you organize your work, and makes your code more easily portable.\nYou can create a new projects from the File tab at upper left, or in the project dropdown menu at upper right. You can just create one in a new directory. Then you can select a name and where you want to save it.\nThere are many different types of projects - this book/website is one!\nIf you want to backup your work with version control or collaborate with others using git and GitHub, you will need to use projects. (Well, technically you don’t need to, but you’d be doing many things manually).",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "rc_git.html",
    "href": "rc_git.html",
    "title": "12  Optional: Git and Github",
    "section": "",
    "text": "If you are interested in:\n\nBacking up your code using a version control system that allows you to roll back changes and monitor incremental progress\nand/or\nSharing your code and collaborating with others\n\nYou may like to try using git (a program for your computer) and GitHub (a website that hosts code projects).\nWe won’t go into detail here, but Jenny Bryan’s excellent introduction and tutorial on the topic can be found here:\nhttps://happygitwithr.com/",
    "crumbs": [
      "R on your Computer",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Optional: Git and Github</span>"
    ]
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "13  The Basics",
    "section": "",
    "text": "13.1 Intro\nI’m sure those of you reading this come from a wide variety of backgrounds regarding computer programming - some of you may be very familiar with it, others total novices. Some of you may love computing, others might hate it. If you’re apprehensive about learning R, or if you find yourself struggling with it - don’t worry! Scientific computing presents a challenge at some point to everyone who does it. Just remember a few things:\nThat said, learning a programming language is a little like learning a human language, except there’s a much smaller vocabulary and the grammar is very strict. And where human language has parts of speech like nouns and verbs, R has a certain syntax as well. Some of the main components of the R language are operators, functions, arguments, and data.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_basics.html#intro",
    "href": "r_basics.html#intro",
    "title": "13  The Basics",
    "section": "",
    "text": "Everyone makes mistakes.\nDon’t be afraid to ask questions!\nDon’t compare yourself to others, compare the “you” of today to the “you” of yesterday.\nEveryone is constantly learning new things, including those who seem like experts.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_basics.html#operators",
    "href": "r_basics.html#operators",
    "title": "13  The Basics",
    "section": "13.2 Operators",
    "text": "13.2 Operators\nOperators are short symbols that tell the computer to do certain simple things. You are already familiar with many operators - the math operators like +, -, *, and /. R at its simplest is a calculator:\n\n## This is block of R code! Anything that starts with # is a comment, and doesn't run.\n\n## adding\n2 + 2\n\n[1] 4\n\n## subtracting\n5 - 4\n\n[1] 1\n\n## multiplying\n3 * 3\n\n[1] 9\n\n## dividing\n6 / 2\n\n[1] 3\n\n\nThere are a couple other math operators too:\n\n## exponentiate with ^\n3^2\n\n[1] 9\n\n## find the remainder with the modulus, %%\n10 %% 3\n\n[1] 1\n\n## perform integer division with %/%\n10 %/% 3\n\n[1] 3\n\n\nBut math operators aren’t the only type! There are also the closely related comparison operators, which will return TRUE or FALSE instead of calculated numbers:\n\n## equals, ==\n2 + 2 == 4\n\n[1] TRUE\n\n## does not equal, !=\n2 + 2 != 4\n\n[1] FALSE\n\n## greater than, &gt;\n5 &gt; 4\n\n[1] TRUE\n\n## less than, &lt;\n5 &lt; 4\n\n[1] FALSE\n\n\nThere are also greater than or equal to (&gt;=) and less than or equal to (&lt;=).\nYou can combine comparisons with logical operators - and (&), or (|), and not (!):\n\n## and: are both true?\n(3 &gt; 2) & (4 &gt; 3) \n\n[1] TRUE\n\n## or: is at least one true?\n(2 == 1) | (4 &lt; 3)\n\n[1] FALSE\n\n## not: is this false?\n!(2 == 1)\n\n[1] TRUE\n\n\nThere are few other important operators, but they will make more sense once we talk about the other parts of R.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "13  The Basics",
    "section": "13.3 Functions",
    "text": "13.3 Functions\nFunctions are words (though not necessarily real words) or letters that instruct the computer to perform more complicated tasks. They generally are followed by parentheses ().\n\n## here's a function that returns the current date\nSys.Date()\n\n[1] \"2024-07-25\"\n\n## and here is a function that returns the date with the time\nSys.time()\n\n[1] \"2024-07-25 11:26:42 CDT\"\n\n\nNo you may be thinking - “this is pretty basic” and “what are the parentheses for?”, which brings use to arguments!",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "13  The Basics",
    "section": "13.4 Arguments",
    "text": "13.4 Arguments\nArguments are values or objects that go inside the parentheses of functions to specify what you want the function to do. This is what gives functions their power. Arguments are separated inside a function by commas.\n\n## the sum function can sum many numbers\nsum(1,2,3,4,5)\n\n[1] 15\n\n\nIn the function above, each number is acting as an argument. In this case, the arguments don’t have names. Oftentimes a function’s arguments will be explicitly named, and to specify what you want those arguments to be, you use the = operator.\n\n## this function pulls values randomly from a normal distribution specified in the arguments\n## n specifies how many numbers to return, and mean and sd specify shape of the distribution\nrnorm(n = 10, mean = 5, sd = 1)\n\n [1] 4.592802 6.464716 5.334547 2.767650 4.871519 4.176753 4.775201 5.181910\n [9] 5.675273 4.735784\n\n\nOperators are actually a special type of function that can be used with syntax that is more intuitive for them. You can also use them in the same way as most functions by surrounding them with back ticks, `.\n\n## here we use the + operator in a much more confusing context\n`+`(2, 2)\n\n[1] 4\n\n## it is equivalent to\n2 + 2\n\n[1] 4",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_basics.html#data",
    "href": "r_basics.html#data",
    "title": "13  The Basics",
    "section": "13.5 Data",
    "text": "13.5 Data\nWe are using the word data here to broadly encompass values (like the numbers we were using above, both with operators and as arguments), variables (stored values), and data structures (organized collections of values).\n\n13.5.1 Values\nValues are much like the data types we discuss in the data analysis section. In fact, the different types of values R can deal with are called data types as well!\nIn R, values can be numeric, character, or logical (among other, more specific types).\n\n## numeric values are numbers!\n2\n\n[1] 2\n\n2.5\n\n[1] 2.5\n\n## character values are letters, words, phrases (often referred to as \"strings\")\n\"a\"\n\n[1] \"a\"\n\n\"apple\"\n\n[1] \"apple\"\n\n\"there is a worm in my apple\"\n\n[1] \"there is a worm in my apple\"\n\n## note: character values or strings must be surrounded by \"\" or '' for R to interpret them as strings\n\n## Logical values are TRUE or FALSE (you've seen these above)\nTRUE\n\n[1] TRUE\n\nFALSE\n\n[1] FALSE\n\n\nThere are other types of values too: missing values (NA and NaN), infinite values (Inf and -Inf), and something that indicates empty (NULL).\n\n\n13.5.2 Variables\nVariables are named values that are stored in the “environment”, or the workspace that R can access to perform its tasks. In order to store a value as a variable, you need to use a special kind of operator called an assignment operator (&lt;- or =). As I mentioned variables have names, which are unquoted text.\n\n## store 2 as a variable called x\nx &lt;- 2\n\n## R returns no output here because you're just storing a value\n## but you can return the value by calling the variable\nx\n\n[1] 2\n\n## store 3 as a variable called y\ny &lt;- 3\n\n## you use variables with operators\nx + y\n\n[1] 5\n\n## store a character value\nstring &lt;- \"hello\"\n\n## math doesn't work on strings\n\nTechnically, you can use = in place of &lt;-. This is why the equals operator is ==. I generally use &lt;- to prevent any confusion between assignment and comparison.\n\n13.5.2.1 Naming Rules\nVariables have rules about how they can be named:\n\nNo special symbols other than _ and .\nYou can’t start with a number or _.\nThey can’t be special words that R interprets differently. You can enter ?Reserved in your console to see a list.\n\n\n\n\n13.5.3 Data Structures\nData structures are collections of values with some sort of organization, and also saved in the environment. Plot twist: the variables above are the simplest data structure, the scalar, which is just a single value.\nThe next data structure is the vector, which is a collection of values of the same data type. We can store them much like variables.\n\n## we use another operator, :, to create a sequence of integers from 1 to 5\nmy_vector &lt;- 1:5\n\nmy_vector\n\n[1] 1 2 3 4 5\n\n## you can also create vectors with the combine function, c()\nmy_other_vector &lt;- c(\"a\", \"b\", \"c\")\n\nmy_other_vector\n\n[1] \"a\" \"b\" \"c\"\n\n\nThe next data structure is called a list. A list is a collection of values like a vector, but they can be of any data type, or data structure. You can have a list of numeric values and character values, a list of vectors, or even a lists of lists! Every other complex data structure is technically a list with special attributes and/or rules.\n\n## you can create lists with the list function\nmy_list &lt;- list(\"a\", 1, 2:4)\n\nmy_list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 2 3 4\n\n## can also use the combine function, but it will default to a vector when data types are the same\nmy_other_list &lt;- c(\"b\", 2)\n\nFinally, the most common special type of list you will use is the data frame. A data frame is a list of vectors that are arranged in a table, much like an excel spreadsheet. Each of the vectors will be named as a column, and all must be the same length. The position of a value in a vector is its row in the data frame.\n\n## we can make a data frame with the data.frame function\nmy_data &lt;- data.frame(letter = c(\"a\",\"b\",\"c\"), # each column has a name\n                      number = c(1, 2, 3),\n                      vowel = c(TRUE, FALSE, FALSE))\n\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c      3 FALSE\n\n\nNext, we will extend these concepts a bit further!",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Basics</span>"
    ]
  },
  {
    "objectID": "r_next.html",
    "href": "r_next.html",
    "title": "14  Next Steps",
    "section": "",
    "text": "14.1 Packages\nPackages are collections of R functions that people write to make tasks easier. One of the strengths of R is that countless programmers have taken the time to assemble functions of use in their respective fields, and shared them with the world. For example the “vegan” package contains a number of functions geared towards community ecology, like calculating diversity indices. You could calculate a diversity index with just the base R, but it would be more difficult and take longer.\nYou can install packages in at least two ways:\nBut Installing packages does not make them automatically accessible to you. When R boots up, it only loads its base functionality by default, so you have to load any packages that you want to use for a given R session. You can do this with the following code (with the package name not in quotes):\nlibrary(PACKAGE NAME)\nThe code for loading packages should be saved in your r script, because it will need to be done every time you open R.\nThere is a family of packages that is very popular called the “tidyverse.” The aim of the tidyverse is to make data manipulation and visualization streamlined and efficient. Some people are very opinionated about whether you should use the tidyverse or base R, but in my opinion, it’s mostly personal preference. If you only want to dip into R and don’t plan to use it much in the future, you may as well just pick up the specific functions you need to use and not worry about much else. If you’d like to continually use R for data analysis, but don’t plan on getting deep into it, getting a handle on the tidyverse may be a good idea. If you want to really get into R, I would recommend learning how to do things in base R (as well as tidyverse functions).\nYou can install the tidyverse suite with:\ninstall.packages(\"tidyverse\")\nMore information on this suite of packages can be found here:\nhttps://www.tidyverse.org/\nNote that when you install the tidyverse, it instalss a large suite of packages, but when you load the tidyverse with the “library” function, it only loads a smaller subset of “core” packages by default. Thus, if you are looking to use a specific tidyverse package with a more niche purpose, you may need to load it separately with another call of the “library” function.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "r_next.html#packages",
    "href": "r_next.html#packages",
    "title": "14  Next Steps",
    "section": "",
    "text": "You can use the following code, with the package names in quotes (this is one of the few times where using the console is recommended, because you only need to install a package once):\n\ninstall.packages(\"PACKAGE NAME HERE\")\n\nOr you can use the packages tab in RStudio. In the lower right panel, there should be a packages tab in between “Plots” and “Help”. Once there, there is an “Install” button. When clicked a window will appear allowing you to search for packages to install.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "r_next.html#subsetting",
    "href": "r_next.html#subsetting",
    "title": "14  Next Steps",
    "section": "14.2 Subsetting",
    "text": "14.2 Subsetting\nIn the last section we introduced data structures. Now let’s talk about what you can do with them.\n\n14.2.1 Vectors\nThe individual elements of a vector can be accessed with bracket operators - [ and ]. You can refer to an element by its index, or its numeric place in the sequence of elements (e.g., the 1st, the 10th, etc.). It’s important to not here that R starts counting at 1, while many other programming language start counting at 0 (e.g., Python). This is another thing that people are opinionated about, and if you put your mind to it, you can be too! Anyway, here are some examples:\n\n## let's create a vector of the first five letters of the alphabet\nmy_vector &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\nmy_vector\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n## now let's return the 5th element\nmy_vector[5]\n\n[1] \"e\"\n\n## we can return multiple elements with c()\nmy_vector[c(2,4)]\n\n[1] \"b\" \"d\"\n\n## or as a series with :\nmy_vector[2:4]\n\n[1] \"b\" \"c\" \"d\"\n\n\nYou can also use negative numbers to exclude values from what’s returned:\n\n## lose the last element\nmy_vector[-5]\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n## everything but the last element\nmy_vector[-1:-4]\n\n[1] \"e\"\n\n\n\n\n14.2.2 Lists\nSubsetting vectors is fairly straightforward, but subsetting lists can be tricky. Since lists have multiple levels of organization, they use both the [] operators and the [[]] operators. Single brackets give you the list element, and double brackets give you what the list element contains. Let’s demonstrate:\n\n## create a list \nmy_list &lt;- list(c(\"a\",\"b\",\"c\"), \"d\", \"e\")\nmy_list\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] \"d\"\n\n[[3]]\n[1] \"e\"\n\n## grab the first list element\nmy_list[1]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n## grab what's conatined in the first list element (in this case a vector)\nmy_list[[1]]\n\n[1] \"a\" \"b\" \"c\"\n\n## another example with a scalar\nmy_list[2]\n\n[[1]]\n[1] \"d\"\n\nmy_list[[2]]\n\n[1] \"d\"\n\n## you can also subset what you have subsetted:\nmy_list[[1]][1]\n\n[1] \"a\"\n\n## but if you try subsetting a list element, it won't work the same way\nmy_list[1][1]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n## this is because [] returns the list element as a list of length 1, therefore [1] gives you the same thing again, and [2] would give you a NULL list.\n\nThis distinction can be difficult to understand, but don’t worry! It takes time. The best analogy I’ve seen is from Hadley Wickham here:\nhttps://adv-r.hadley.nz/subsetting.html#subset-single\nYou can think of a list as a train, every list element is a train car, and each has its own contents. Single brackets give you the train car/s, and double brackets gives you what’s inside a single train car. And even a single train car can be another train (or a list). Also note:\n\n## you can grab multiple list elements with []; this give a list with two elements\nmy_list[1:2]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] \"d\"\n\n## list elements can be named and indexed by their name as well\nnamed_list &lt;- list(first = 1:3, second = 10)\nnamed_list\n\n$first\n[1] 1 2 3\n\n$second\n[1] 10\n\nnamed_list[\"first\"]\n\n$first\n[1] 1 2 3\n\n\n\n\n14.2.3 Data Frames\n\n14.2.3.1 Base\nSubsetting data frames is a little easier to get a handle on, you just need to think in two dimensions. When using single brackets to subset data frames, you need to specify the index of the row and the column separately and in that order. You separate each index number by a comma inside the brackets. Check it out:\n\n## create data frame\nmy_data &lt;- data.frame(letter = c(\"a\",\"b\",\"c\"), # each column has a name\n                      number = c(1, 2, 3),\n                      vowel = c(TRUE, FALSE, FALSE))\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c      3 FALSE\n\n## grab the element in the 2nd row, 1st column\nmy_data[2,1]\n\n[1] \"b\"\n\n## you can also grab a whole row or column by leaving onse side of the comma blank\nmy_data[2,]\n\n  letter number vowel\n2      b      2 FALSE\n\nmy_data[,1]\n\n[1] \"a\" \"b\" \"c\"\n\n## (subsetting a row gives you a data frame, subsetting a column gives you a vector)\n\nBut data frames also have named columns! Let’s use that to our advantage. You can specify a column’s name instead of its index in brackets, like for a list, or you can use the $ operator.\n\n## subsetting by name in brackets\nmy_data[,\"vowel\"]\n\n[1]  TRUE FALSE FALSE\n\n## subsetting by name with $ (notice no quotes)\nmy_data$vowel\n\n[1]  TRUE FALSE FALSE\n\n## the downside of $ is that you can't grab more than one column like with brackets\nmy_data[,c(\"letter\", \"vowel\")]\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## subsetting multiple columns gives you a data.frame\n\n## you can use $ with named lists too\nnamed_list$first\n\n[1] 1 2 3\n\n## you can mix subsetting operators if you ever need to\nmy_data$vowel[1]\n\n[1] TRUE\n\nmy_data[1,]$vowel\n\n[1] TRUE\n\n\nYou can also use brackets to select rows by value, not index. You just need to use some comparison operator in a statement that resolves as TRUE or FALSE.\n\n## grab the consonant rows\nmy_data[my_data$vowel == FALSE,]\n\n  letter number vowel\n2      b      2 FALSE\n3      c      3 FALSE\n\n## grab the rows before the third\nmy_data[my_data$number &lt; 3,]\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n\n## you can combine criteria\nmy_data[my_data$number &lt; 3 & my_data$vowel == FALSE,]\n\n  letter number vowel\n2      b      2 FALSE\n\n\n\n\n14.2.3.2 tidyverse\nNow the reason that we talked about packages in between data structures and subsetting is because the tidyverse (specifically, the dplyr package) has more functions for subsetting: filter and select. Filter works much like grabbing rows by value, and select works like grabbing columns by name. Let’s look at some examples:\n\n## load the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## filter for consonants\nfilter(.data = my_data, vowel == FALSE)\n\n  letter number vowel\n1      b      2 FALSE\n2      c      3 FALSE\n\n## select letter related columns\nselect(.data = my_data, letter, vowel)\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## you can also exclude columns\nselect(.data = my_data, !number)\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## note: selecting a single column will give a data frame, not a vector\nselect(.data = my_data, number)\n\n  number\n1      1\n2      2\n3      3\n\n## another tidyverse/dplyr function, pull, will give just a vector\npull(.data = my_data, number)\n\n[1] 1 2 3\n\n\nAs you can see, filter, select and pull are versatile, consistent and powerful. However, they lack one key ability: assignment. You can use brackets and $s to assign things (which are base R operators):\n\n## assign a new value to a data element (NA means missing value)\nmy_data[3,2] &lt;- NA\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c     NA FALSE\n\n## create a whole new column with $ (vector must be of same length as the number of rows)\nmy_data$new_column &lt;- c(\"some\", \"new\", \"data\")\nmy_data\n\n  letter number vowel new_column\n1      a      1  TRUE       some\n2      b      2 FALSE        new\n3      c     NA FALSE       data",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Next Steps</span>"
    ]
  },
  {
    "objectID": "r_import.html",
    "href": "r_import.html",
    "title": "15  Importing Data",
    "section": "",
    "text": "15.1 Reading Data\nImporting data into R is often referred to as reading data, as that is what the computer is doing, it’s reading the contents of a file (usually a text file). Most ecologists and data scientists work with a text file called a Comma Separated Value file, or csv. This is a small file that’s easy for computers to read where each column is separated by a column, and each row by a new line. You can save excel files as csv from the “Save As…” menu, and you can specify csv as the type when downloading a Google sheet.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "r_import.html#reading-data",
    "href": "r_import.html#reading-data",
    "title": "15  Importing Data",
    "section": "",
    "text": "15.1.1 From Your Computer\nSo when you have files you want to read locally on your computer, the first thing you need to think about is what’s called the “working directory”. The working directory is the folder on your computer where R will look for files when prompted, and also where it will save output.\nYou can check your current working directory:\n\n## return current working directory\ngetwd()\n\n[1] \"/Users/kit/Documents/UMN/Research/cedar_creek_projects/ccesr_intern_hub\"\n\n\nYou can also set your working directory manually\n\n## change working directory\nsetwd(\"some/different/folder\")\n\nOr, in RStudio, you can click the Session dropdown menu at the top of the window, then “Set Working Directory”, then “Choose Directory.”\nIf you use an R Project (highly recommended), you don’t have to worry as much about this. If you have a project open, the working directory will be automatically set to the folder that contains the .Rproj file that is created when you create a project. See Section 11.2 for more info!\nWhen you’re in an R Project, or have a csv you want in your working directory, you can read it into your environment like so:\n\n## read data\nmy_data &lt;- read.csv(\"the name of your file in quotes\", header = TRUE)\n\nThe read.csv function creates a data frame from the csv you specify, and then the &lt;- assigns it to “my_data.” The “header = TRUE” argument tells R to interpret the first line of the csv as the column names.\n\n\n15.1.2 tidyverse Function\nThe readr package in the tidyverse family also has its own data reading functions.\n\n## load tidyverse\nlibrary(tidyverse)\n\n## read data (assumes header by default)\nmy_data &lt;- read_csv(\"name of your data in quotes\")\n\nThese functions are pretty similar, with one exception: read.csv gives you a data frame, but read_csv gives you a “tibble.” What is a tibble? It’s another special type of list, much like a data frame, but with a few differences. It was designed to work more consistently with tidyverse functions. One important difference between data frames and tibbles that when you subset an individual column with the brackets ([]), data frames will give you vectors, and tibble will give single column tibbles. This has caused me confusion when writing functions, but you may not run into it.\n\n\n15.1.3 From The Web\nYou can also read files directly from the web. If you have your data in Google sheets, you can create a URL for R to import it directly. Simply go to the File menu, click “Share” and then “Publish to web”. In the box that pops up, you will need to select the file type as “.csv”, not web page. Then save the URL that it gives you!\nFor demonstration, I’ve created a few data sheets that you too can import into R by copying the following code:\n\n## put the url of the data in quotes\nfake_mammals &lt;- read.csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQ9mfx88nM33PC6WpIh3nSxMvkM98nEszw5gpUq7KdqbiCskF8Pqvrl0W2EqNf9rD1JEepb-hSMIb_j/pub?output=csv\", header = TRUE)\n\nfake_insects &lt;- read.csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0snHMdsxzzzkxt_JVRFooJDB60lGSJQlrjUU29tGYOhIpqvx_pzja3Eqr9l5b4f76yMFvkiGzuK1Z/pub?output=csv\")\n\nThese two files will be used throughout the next chapters. The first is some made-up data of some mammal captures at 6 sites across forest and savanna habitats (with mass and parasite info), and the second is made up sweep-netting data from the same sites.\n\n\n15.1.4 Other File Types\nNow there may be times when you want or need to import data that aren’t .csv files. You may come across .tsv files, which are “tab-separated values” files. These are also in a text based format, but use tabs instead of commas to delimit columns. In Base R, the “read.delim” function will read .tsv files by default. The tidyverse readr package has “read_tsv” for this purpose. You can also use “read.delim” or readr’s “read_delim” to read a text file with any sort of delimiter between columns, like dashes, underscores, dots, etc. The “read.delim” function can do this by setting the “sep” argument, and the “read_delim”function can do it by setting the “delim” argument.\nYou may also want to import Microsoft Excel files (.xls or .xlsx). Base R doesn’t have any functionality for this, but the tidyverse’s readxl package does: the “read_excel” function. Do note however that you may need to specify the sheet you want! Also, the readxl package is not in the “core” tidyverse, so you will have to specifically load it in R.\nMore info on the tidyverse’s readr and readxl can be found here:\nhttps://readr.tidyverse.org/\nhttps://readxl.tidyverse.org/\nYou can also import Google Sheets with the googlesheets4 package, but that is is a bit trickier given that you’ll need to specify your Google credentials and work with Google Drive folder hierarchy and file IDs. The googlesheets4 package, as well as the related googledrive package, are both in the tidyverse family, but not in the “core”, so need to be specifically loaded. More info here:\nhttps://googlesheets4.tidyverse.org/\nhttps://googledrive.tidyverse.org/",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "r_import.html#checking-data",
    "href": "r_import.html#checking-data",
    "title": "15  Importing Data",
    "section": "15.2 Checking Data",
    "text": "15.2 Checking Data\nNow that you have data, you will want to look at it!\n\n15.2.1 The Whole Table\nYou can look at a whole data frame by clicking on its name in the “Environment” pane in RStudio (upper right), or with the View() function:\n\nView(fake_mammals)\n\nYou can also just look at parts:\n\n## check top 6 rows\nhead(fake_mammals)\n\n  site site_type            species mass_g tick_count helminth_mass_mg\n1    a    forest White-footed mouse     20          0              512\n2    a    forest White-footed mouse     24         10              365\n3    a    forest White-footed mouse     23          2                0\n4    a    forest White-footed mouse     19          0              608\n5    a    forest White-footed mouse     25         12              109\n6    a    forest         Deer mouse     22          3              456\n\n## check bottom 6 rows\ntail(fake_mammals)\n\n   site site_type            species mass_g tick_count helminth_mass_mg\n43    f   savanna White-footed mouse     21          0              408\n44    f   savanna White-footed mouse     25          1              197\n45    f   savanna White-footed mouse     24          0              152\n46    f   savanna         Deer mouse     20          0              508\n47    f   savanna         Deer mouse     22          2              496\n48    f   savanna        Meadow vole     23         NA               56\n\n\nYou can also take a look at the structure of the data with str(), which will tell you how many rows (observations) and how many columns (variables), as well as the type of each column.\n\n## check structure\nstr(fake_mammals)\n\n'data.frame':   48 obs. of  6 variables:\n $ site            : chr  \"a\" \"a\" \"a\" \"a\" ...\n $ site_type       : chr  \"forest\" \"forest\" \"forest\" \"forest\" ...\n $ species         : chr  \"White-footed mouse\" \"White-footed mouse\" \"White-footed mouse\" \"White-footed mouse\" ...\n $ mass_g          : int  20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int  0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int  512 365 0 608 109 456 521 432 20 129 ...\n\n\n\n\n15.2.2 Individual Columns\nYou can also take a look at individual columns with the $ operator, and get quick summaries with summary():\n\n## summarize mammal masses\nsummary(fake_mammals$mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  17.00   20.75   23.00   22.60   25.00   28.00 \n\n## summarize helminth masses\nsummary(fake_mammals$helminth_mass_mg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   119.5   341.5   306.5   501.2   713.0 \n\n\nNote that for the second summary (helminth mass), it tells you how many NA’s, or missing values, there are.\n\n\n15.2.3 Factors: The Pseudo Data Type\nUnless you specify, csv reading functions will assume the data type of each column in a data sheet (numeric, character, etc.). Any categorical variable will be considered a character type generally. But the way character types are stored in computer memory does not lend itself well to statistical analysis. To remedy this, R has a special data type called the factor for categorical data. A factor is made up of two parts - the levels, which are stored to the computer as integers, and the labels, which are character strings that we can read as the category names.\nIn our mammal data, we probably want mammal species, site type, and site all to be factors. We can convert them with the as.factor function!\n\n## convert species to factor\nfake_mammals$species &lt;- as.factor(fake_mammals$species)\n## convert site type to factor\nfake_mammals$site_type &lt;- as.factor(fake_mammals$site_type)\n## convert site to a factor\nfake_mammals$site &lt;- as.factor(fake_mammals$site)\n\n## check it out!\nsummary(fake_mammals$species)\n\n        Deer mouse        Meadow vole White-footed mouse \n                15                  9                 24 \n\n## also do for insects\nfake_insects$site &lt;- as.factor(fake_insects$site)\nfake_insects$site_type &lt;- as.factor(fake_insects$site_type)\n\nThere are also similar functions for converting data types to numeric (as.numeric()) and character (as.character()).",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data</span>"
    ]
  },
  {
    "objectID": "r_wrangle.html",
    "href": "r_wrangle.html",
    "title": "16  Wrangling Data",
    "section": "",
    "text": "16.1 Adding Columns\nOne simple thing you may want to do is add columns to your data, which may be calculations from existing columns.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "r_wrangle.html#adding-columns",
    "href": "r_wrangle.html#adding-columns",
    "title": "16  Wrangling Data",
    "section": "",
    "text": "16.1.1 Base\nIn base R, we have already kinda done this. You can assign something to a new column with the $ and &lt;- operators.\nFor the insect data, let’s say we wanted to calculate average temperature at a given site based on the recorded high (temp_hi) and low (temp_low):\n\n## calculate mean temp\nfake_insects$temp_mean &lt;- (fake_insects$temp_hi + fake_insects$temp_lo)/2\n\n## check it out\nfake_insects$temp_mean\n\n[1] 21.25 23.25 19.25 26.75 24.00 28.00\n\n\n\n\n16.1.2 tidyverse\nIn the tidyverse, adding new columns is done with the mutate function:\n\n## load tidyverse\nlibrary(tidyverse)\n\n## mutate a new column\nfake_insects &lt;- mutate(.data = fake_insects, ## specify data\n                       temp_mean_mutated = (temp_hi + temp_lo)/2) ## calculate new column\n\n## this column should be the same for all six rows (a TRUE should be returned for each)\nfake_insects$temp_mean == fake_insects$temp_mean_mutated\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nAs you can see, using mutate mean you have to write the name of data frame fewer times.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "r_wrangle.html#pivoting-reshaping",
    "href": "r_wrangle.html#pivoting-reshaping",
    "title": "16  Wrangling Data",
    "section": "16.2 Pivoting / Reshaping",
    "text": "16.2 Pivoting / Reshaping\nYou also may need to transform your data between the wide and long formats (see 2.1 Data Formats). I find that the pivot functions from tidyr in the tidyverse are easier to use, so we will go over those. But by all means, if you prefer base, go for it! I’m just less familiar with using base R for this.\n\n16.2.1 Wide to Long\nThe insect data is partly in wide format: we have a column for each order of insect, where the count is implicitly the values in the cells. It will be easier to work with if we make one “count” column and one “order” column. We can do this with the pivot_longer function:\n\n## lengthen the order count data\nlong_insects &lt;- pivot_longer(data = fake_insects, \n                             cols = c(hymenoptera, lepidoptera, coleoptera, diptera, \n                                      odonata, hemiptera, orthoptera, ephemeroptera, \n                                      tricoptera, plecoptera),\n                             names_to = \"order\",\n                             values_to = \"count\"\n                             )\n\n## check it out\nhead(long_insects)\n\n# A tibble: 6 × 8\n  site  site_type temp_hi temp_lo temp_mean temp_mean_mutated order       count\n  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n1 a     forest       23.5      19      21.2              21.2 hymenoptera     9\n2 a     forest       23.5      19      21.2              21.2 lepidoptera     3\n3 a     forest       23.5      19      21.2              21.2 coleoptera     16\n4 a     forest       23.5      19      21.2              21.2 diptera        29\n5 a     forest       23.5      19      21.2              21.2 odonata         4\n6 a     forest       23.5      19      21.2              21.2 hemiptera      10\n\n\nNow we have multiple rows for each site, one for each order! You may not believe me, but this will make things easier down the line.\nFor reference, the cols argument specifies which columns you want to pivot. The names_to argument names the column that will contain the pivoted column names, and the values_to argument names the column in which the cell values will be placed.\n\n\n16.2.2 Long to Wide\nYou’ll occasionally want to turn long to wide as well. pivot_wider works for this\n\n## widen our long data\nwide_insects &lt;- pivot_wider(data = long_insects,\n                            names_from = \"order\",\n                            values_from = \"count\")\n\nThis is the inverse of what we just did - we made a column for each value in the “order” column given to the names_from argument, the values of which are pulled from the “count” column given to the values_from argument.\n\n\n16.2.3 Other Data Reformatting Functions\nThere are other functions that can be used to reformat data between long and wide, but some are outdated. You may come across some of them in blogs of code examples.\nThe base R function for transforming data between wide and long format is “reshape”, from the “stats” package. It’s just one function that can transform data in both directions. More info here:\nhttps://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/reshape\nThere are also previous versions of the “pivot” functions, such as\n\n“cast” and “melt”, from the “reshape” package\n“dcast”, “acast”, and “melt”, from the “reshape2” package\n“gather” and “spread”, from older versions of the “tidyr” package\n\nAll of the above still work, but have been superseded by the pivot functions.\nPersonally, I prefer to use the pivot functions because they are named as exactly what they do and they work nicely with other tidyverse functions. The reshape function from base R works fine too, but may have some slight differences. I recommend avoiding using the older cast, melt, gather, or spread functions.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "r_wrangle.html#string-manipulation",
    "href": "r_wrangle.html#string-manipulation",
    "title": "16  Wrangling Data",
    "section": "16.3 String Manipulation",
    "text": "16.3 String Manipulation\nString manipulation may sound like playing cat’s cradle or fingerstyle guitar, but in the context of R programming, it refers to working with text data. As you may recall, values with the character data type in R can also be called “strings”. Essentially, strings ar strings of characters (alphanumeric, symbols, etc.).\nIn ecological and environmental data, we often encounter strings as names of sites or plots, as labels for categorical data (which we transform into factors), or as notes on observations.\nR has many functions for working with strings - combining them, splitting them, searching through them, transforming them, padding them with leading zeroes, etc. We will focus on a few simple ones that might be useful for typical ecological data workflows. Base R features several valuable functions that can do these tasks. In addition, the “stringr” package in the tidyverse features a suite of consistently formatted functions that do similar things (stringr is part of the core tidyverse, so is loaded with the “library(tidyverse)” command). If you are getting deep into pattern recognition, string transformation, etc., the stringr package is probably your best bet, but for the simple things we’re about to go over, base and tidverse functions both work fine.\nNote: From a broad perspective, R is not the language of choice when it comes to text data, as it was designed primarily for statistics. In the realms of bioinformatics (genomes are really long strings!) and language processing (the written word is made of strings!), Python is often used. But for most ecological purposes, R does just fine.\n\n16.3.1 Combining Strings\nOften times you may want to combine several columns of your data into an “ID” type variable to describe observations from a certain plot in a certain site on a particular date. The base R “paste” function and then less well-named “str_c” function from the stringr package can do this easily.\n\n## first let's make some fake data to work with\n## this will include sites , plots, and years of observation\nfake_observations &lt;- data.frame(site = c(1, 1, 2, 2, 1, 1, 2, 2),\n                                plot = c(\"a\", \"b\", \"a\", \"b\", \n                                         \"a\", \"b\", \"a\", \"b\"),\n                                year = c(2023, 2023, 2023, 2023,\n                                         2024, 2024, 2024, 2024))\n\n## now let's add an ID column by combining these variables\n\n## first with paste\n## we specify we want each variable to be separated by an underscore\n## with the \"sep\" argument\nfake_observations$id &lt;- paste(fake_observations$site,\n                              fake_observations$plot,\n                              fake_observations$year,\n                              sep = \"_\") \n\n## now look at the result\nfake_observations$id\n\n[1] \"1_a_2023\" \"1_b_2023\" \"2_a_2023\" \"2_b_2023\" \"1_a_2024\" \"1_b_2024\" \"2_a_2024\"\n[8] \"2_b_2024\"\n\n## now let's try with str_c\n## this time we use a hyphen in the \"sep\" argument\nfake_observations &lt;- mutate(fake_observations,\n                            id2 = str_c(site, plot, year, sep = \"-\"))\n\n## and we can look at this as well (using pull from tidyverse this time)\npull(fake_observations, id2)\n\n[1] \"1-a-2023\" \"1-b-2023\" \"2-a-2023\" \"2-b-2023\" \"1-a-2024\" \"1-b-2024\" \"2-a-2024\"\n[8] \"2-b-2024\"\n\n\nNote that for both functions we first present the variables we want to combine, and then specify how we want each to be separated in the resulting string. Also note that “site” and “year” are numeric data in the original data frame, but can be combined into strings all the same.\nThe paste function and str_c function behave similarly in most simple cases, but have slightly different rules when it comes to dealing with missing values (NAs) and when input vectors have different lengths.\n\n\n16.3.2 Converting String Case\nSometimes when entering ecological data, character strings might end up with inconsistent capitalization. As a result, when converting a categorical variable into a factor, you can end up with multiple levels for the same category. For example, if you have four plots at a site- “north”, “east”, “south”, and “west”, and some entries of the “north” plot are written as “North”, the resulting factor would have both a “north” level and a “North” level. Converting your strings all to one case can avoid this.\nYou can convert to upper case with:\n\n“toupper”, the base R function\n“str_to_upper”, the stringr function\n\nAnd you can convert to lower case with:\n\n“tolower”, the base R function\n“str_to_lower”, the stringr function\n\nHere is an example:\n\n## create a vector of character strings to work with\nplots &lt;- c(\"north\", \"North\", \"east\", \"east\",\n           \"South\", \"South\", \"West\", \"west\")\n\n## convert to factor\nplot_factor &lt;- as.factor(plots)\n\n## check out the factor\nsummary(plot_factor)\n\n east north North South  west  West \n    2     1     1     2     1     1 \n\n## oh no, there are six levels instead of four!\n\n## let's fix it with \"tolower\"\nplots_lower &lt;- tolower(plots)\n\n## and factorize...\nlower_plot_factor &lt;- as.factor(plots_lower)\n\nsummary(lower_plot_factor)\n\n east north south  west \n    2     2     2     2 \n\n## much better!\n\nFor more information see the reference site for the stringr package:\nhttps://stringr.tidyverse.org/\nAnd this article comparing stringr and base R string functions:\nhttps://stringr.tidyverse.org/articles/from-base.html#mutate-strings",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wrangling Data</span>"
    ]
  },
  {
    "objectID": "r_summarize.html",
    "href": "r_summarize.html",
    "title": "17  Summarizing Data",
    "section": "",
    "text": "17.1 Describing With Summaries\nYou’ll often want to give simple, illustrative information about the data you collected. The tidyverse is great for this! (base R has “aggregate”, which also works, but we won’t go into it here).",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "r_summarize.html#describing-with-summaries",
    "href": "r_summarize.html#describing-with-summaries",
    "title": "17  Summarizing Data",
    "section": "",
    "text": "17.1.1 tidyverse\nThe package dplyr in the tidyverse has two wonderful functions: group_by() and summarize(). You can also use the British spelling, “summarise()”, but I use a z because it’s what the founders would have wanted.\nBefore we use these however, we need to introduce a very useful operator, the pipe: %&gt;%. This operator directs data into the first argument of a function, which allows you to chain functions together efficiently. Let’s try an example with the filter and select subsetting functions (see 14.2.3 Next Steps; Subsetting; Data Frames):\n\n## grab only the forest sites from the insect data\nforest_sites &lt;- fake_insects %&gt;% ## take fake_insects and pipe it into filter...\n  filter(site_type == \"forest\") %&gt;% ## filter only forest rows, pipe into select\n  select(site) ## select only the site column\n## the whole pipe chain is assigned to \"forest_sites\"\n\nforest_sites\n\n  site\n1    a\n2    b\n3    c\n\n## this is the same as\nforest_rows &lt;- filter(fake_insects, site_type == \"forest\")\nforest_sites &lt;- select(forest_rows, site)\n\nNote: base R also has a pipe operator, |&gt;. It’s newer and mostly the same as %&gt;%, so I just haven’t transitioned.\nNow, let’s try with group_by() and summarize()! Let’s say you wanted the total number of insects caught at each site (be sure to have pivoted your insect data as described in the setup above!):\n\n## summarize total insect catch\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site) %&gt;% ## group observations\n  summarize(total_insects = sum(count)) ## sum all insects\n\ninsect_counts\n\n# A tibble: 6 × 2\n  site  total_insects\n  &lt;fct&gt;         &lt;int&gt;\n1 a                84\n2 b                87\n3 c               136\n4 d                60\n5 e                61\n6 f                51\n\n\nAs you can see, the summarize function works a bit like the mutate function, in that you create a new column.\nNote that group_by doesn’t visibly change your data, but it changes some attributes that the computer can see when it runs the summarize function. If you forgot which sites are in which type of habitat, you could also include that variable in the group_by arguments (since it doesn’t subdivide the sites, it won’t change the calculation).\n\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site, site_type) %&gt;%\n  summarize(total_insects = sum(count))\n\nYou can also calculate means and variances! You can use the mean, var, and sd functions. Let’s try for each order across all sites:\n\norder_summary &lt;- long_insects %&gt;%\n  group_by(order) %&gt;%\n  summarize(count_mean = mean(count), ## you can do multiple sumaries at once\n            count_var = var(count),\n            count_sd = sd(count))\n\nhead(order_summary)\n\n# A tibble: 6 × 4\n  order         count_mean count_var count_sd\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 coleoptera         15.8      43.4      6.59\n2 diptera            17       163.      12.8 \n3 ephemeroptera       5.33    171.      13.1 \n4 hemiptera           6.17     42.6      6.52\n5 hymenoptera         8.5       9.1      3.02\n6 lepidoptera         5.67      5.87     2.42\n\n\nYou could also do this separately by site type:\n\norders_by_habitat &lt;- long_insects %&gt;%\n  group_by(site_type, order) %&gt;%\n  summarize(count_mean = mean(count), ## you can do multiple sumaries at once\n            count_var = var(count),\n            count_sd = sd(count))\n\n`summarise()` has grouped output by 'site_type'. You can override using the\n`.groups` argument.\n\nhead(orders_by_habitat)\n\n# A tibble: 6 × 5\n# Groups:   site_type [1]\n  site_type order         count_mean count_var count_sd\n  &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 forest    coleoptera         20.7      17.3      4.16\n2 forest    diptera            28.3       9.33     3.06\n3 forest    ephemeroptera      10.7     341.      18.5 \n4 forest    hemiptera          11.7      14.3      3.79\n5 forest    hymenoptera        10.3       2.33     1.53\n6 forest    lepidoptera         4.33      2.33     1.53\n\n\n\n\n17.1.2 Saving Summaries\nFinally, you can save summarized output with base R’s write.csv() or write_csv() from the tidyverse:\n\n## save summary to your working/project directory\n## first argument is data, second argument is filename\nwrite.csv(orders_by_habitat, \"order_summary\")\n\nWhen saving output, it can be often helpful to add a timestamp to the saved file name, so you can easily identify when you created it and sort among versions. This is another time when we can use the string combining functions we learned about earlier! (see 16.3.1 String Manipulation, Combining Strings)\n\n## first let's save a timestamp string with \"Sys.time\"\n## I like to format it with the \"format\" function \n## this code results in \"YYYYMMDD_HHMMSS\" format\n## (don't worry about the specifics, but feel free to use this code)\ntimestamp &lt;- format(Sys.time(), format = \"%Y%m%d_%H%M%S\")\n\n## then we can use it in our data writing step:\nwrite.csv(orders_by_habitat, paste(\"order_summary\", timestamp, sep = \"_\"))\n\n\n\n17.1.3 Making Pretty Tables\nThe “gt” package is good for this (quick tutorial under construction….)\nhttps://gt.rstudio.com/",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "r_summarize.html#community-ecology",
    "href": "r_summarize.html#community-ecology",
    "title": "17  Summarizing Data",
    "section": "17.2 Community Ecology",
    "text": "17.2 Community Ecology\nAverages and variances are all well and good but what about ecological measures?\n\n17.2.1 Richness\nYou may be interested in how many insect orders are represented in each site.\nLet’s do it in a pipe chain!\n\n## order presence\norder_richness_site &lt;- long_insects %&gt;% \n  mutate(presence = as.numeric(count &gt; 0)) %&gt;% ## create binary presence column\n  group_by(site, site_type) %&gt;%\n  summarize(order_richness = sum(presence))\n\n`summarise()` has grouped output by 'site'. You can override using the\n`.groups` argument.\n\norder_richness_site\n\n# A tibble: 6 × 3\n# Groups:   site [6]\n  site  site_type order_richness\n  &lt;fct&gt; &lt;fct&gt;              &lt;dbl&gt;\n1 a     forest                 8\n2 b     forest                 7\n3 c     forest                 9\n4 d     savanna                6\n5 e     savanna                7\n6 f     savanna                6\n\n\nI calculated the presence column by checking if each value is positive (&gt; 0), which returns a logical TRUE or FALSE, and then if you convert a logical variable to a numeric variable, TRUEs become 1s and FALSEs become 0s. Nifty!\n\n\n17.2.2 Diversity\nvegan package (under construction…)",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "r_summarize.html#related-topic-joining-data",
    "href": "r_summarize.html#related-topic-joining-data",
    "title": "17  Summarizing Data",
    "section": "17.3 Related Topic: Joining Data",
    "text": "17.3 Related Topic: Joining Data\nSometimes with summaries, you will want to connect them to other pieces of data. Here we have some insect counts by site, and some mammal data by site. Let’s connect them! We can use the “merge” function from base R or the “join” functions from the tidyverse.\nWith merge:\n\n## grab only the site and total columns from insect_counts\n## this prevent doubling the site_type column\nmerged_data &lt;- merge(insect_counts[,c(\"site\", \"total_insects\")], fake_mammals, by = \"site\")\n\n## look at the new column in your data\nstr(merged_data)\n\n'data.frame':   48 obs. of  7 variables:\n $ site            : Factor w/ 6 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ total_insects   : int  84 84 84 84 84 84 84 84 84 84 ...\n $ site_type       : Factor w/ 2 levels \"forest\",\"savanna\": 1 1 1 1 1 1 1 1 1 1 ...\n $ species         : Factor w/ 3 levels \"Deer mouse\",\"Meadow vole\",..: 3 3 3 3 3 1 1 1 2 2 ...\n $ mass_g          : int  20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int  0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int  512 365 0 608 109 456 521 432 20 129 ...\n\n\nWith join:\n\n## there are different join functions for different contexts\n## left_join keeps every row from the first data frame and adds any matching rows from the\n## second. it works in most cases\n## inner_join and full_join can also be useful\njoined_data &lt;- insect_counts %&gt;%\n  select(total_insects, site) %&gt;%\n  left_join(fake_mammals, by = \"site\")\n\n## look at it\nstr(joined_data)\n\ntibble [48 × 7] (S3: tbl_df/tbl/data.frame)\n $ total_insects   : int [1:48] 84 84 84 84 84 84 84 84 84 84 ...\n $ site            : Factor w/ 6 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ site_type       : Factor w/ 2 levels \"forest\",\"savanna\": 1 1 1 1 1 1 1 1 1 1 ...\n $ species         : Factor w/ 3 levels \"Deer mouse\",\"Meadow vole\",..: 3 3 3 3 3 1 1 1 2 2 ...\n $ mass_g          : int [1:48] 20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int [1:48] 0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int [1:48] 512 365 0 608 109 456 521 432 20 129 ...",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing Data</span>"
    ]
  },
  {
    "objectID": "r_analyze.html",
    "href": "r_analyze.html",
    "title": "18  Analyzing Data",
    "section": "",
    "text": "18.1 Making Comparisons\nFirst off, let’s just do some simple comparisons.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analyzing Data</span>"
    ]
  },
  {
    "objectID": "r_analyze.html#making-comparisons",
    "href": "r_analyze.html#making-comparisons",
    "title": "18  Analyzing Data",
    "section": "",
    "text": "18.1.1 t-tests\nLet’s say we want to compare two groups, like the number of insects caught in forests and savannas. We already created a summary of this in the last chapter:\n\n## sum insects by site\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site, site_type) %&gt;%\n  summarize(total_insects = sum(count))\n\n`summarise()` has grouped output by 'site'. You can override using the\n`.groups` argument.\n\ninsect_counts\n\n# A tibble: 6 × 3\n# Groups:   site [6]\n  site  site_type total_insects\n  &lt;fct&gt; &lt;fct&gt;             &lt;int&gt;\n1 a     forest               84\n2 b     forest               87\n3 c     forest              136\n4 d     savanna              60\n5 e     savanna              61\n6 f     savanna              51\n\n\nIt seems that there may be a difference! So let’s run a t-test to test for a difference in means between two groups (see 9.1.2.1 Two Predictor Categories). Most stats functions in R can use the formula operator, ~. This allows us to connect our dependent variable (insect count in this case) as a function of our independent variable (site habitat type): total_insects ~ site_type.\n\n## run the t test\nhabitat_comparison &lt;- t.test(formula = insect_counts$total_insects ~ insect_counts$site_type)\n\n## check the output\nhabitat_comparison\n\n\n    Welch Two Sample t-test\n\ndata:  insect_counts$total_insects by insect_counts$site_type\nt = 2.6235, df = 2.1422, p-value = 0.1116\nalternative hypothesis: true difference in means between group forest and group savanna is not equal to 0\n95 percent confidence interval:\n -24.30218 114.30218\nsample estimates:\n mean in group forest mean in group savanna \n            102.33333              57.33333 \n\n\nIf we look at the ouput, it look like the forest mean was 102.333, and the savanna mean was 57.333, for a mean difference or effect size of 45. The p-value, or how strong the evidence for a relationship is, is 0.1116. This is higher than the traditional threshold for significance, likely because we have a very small sample size (6 total).\nNote: if you check the help for the t.test function (run ?t.test), you can find arguments for paired t-tests (paired) and unequal variances among groups (var.equal).\n\n\n18.1.2 ANOVA\nWhat if we have more than two categories, and we want to see if any two categories have different means? Let us return to the mammal data and compare the mass of helminths (parasitic worms) in different mammal species. We can run an analysis of variance (see 9.1.2.2 More Than Two Predictor Categories).\n\n## run the anova\nhelminth_comparison &lt;- aov(fake_mammals$helminth_mass_mg ~ fake_mammals$species)\n\n## check the output (now with the summary function)\nsummary(helminth_comparison)\n\n                     Df  Sum Sq Mean Sq F value  Pr(&gt;F)    \nfake_mammals$species  2  719721  359861   10.59 0.00017 ***\nResiduals            45 1529035   33979                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHey, that’s a small p-value (0.00017)! That means we have strong evidence that there is at least one difference among the pairs of species, either between white-footed mice and deer mice, white-footed mice and meadow voles, or deer mice and meadow voles. We can use a Tukey’s test to find out more:\n\n## run tukey on the anova output\nhelminth_tukey &lt;- TukeyHSD(helminth_comparison)\n\n## check it out\nhelminth_tukey\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fake_mammals$helminth_mass_mg ~ fake_mammals$species)\n\n$`fake_mammals$species`\n                                    diff        lwr        upr     p adj\nMeadow vole-Deer mouse         -356.2222 -544.58911 -167.85534 0.0001061\nWhite-footed mouse-Deer mouse  -111.3333 -258.37719   35.71052 0.1698916\nWhite-footed mouse-Meadow vole  244.8889   70.26811  419.50966 0.0039996\n\n\nAt the bottom here we can see the pairwise comparisons. The two mouse species differ in helminth mass by ~111mg, but the difference is not significant. Meadow voles have a significantly different mean helminth mass from both mouse species. So in the data I made up, voles have less helminth mass than mice.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analyzing Data</span>"
    ]
  },
  {
    "objectID": "r_analyze.html#assessing-relationships",
    "href": "r_analyze.html#assessing-relationships",
    "title": "18  Analyzing Data",
    "section": "18.2 Assessing Relationships",
    "text": "18.2 Assessing Relationships\nBut what if you’re not dealing with categorical comparisons? Then we can check for numerical associations.\n\n18.2.1 Correlation\nWe can look for simple associations without cause and effect with correlations (see 9.1.3.1 Simple Association). Mice seem to have high helminth loads, so let’s check for a correlation between their body mass and helminth mass:\n\n## create a subset of only mouse data\n## I use the %in% operator to specify that species should be found in a specified vector\n## AKA, it could be white-footed mouse OR deer mouse\nmouse_data &lt;- filter(fake_mammals, species %in% c(\"White-footed mouse\", \"Deer mouse\"))\n\n## run correlation with two variables (no formula here)\ncor.test(mouse_data$helminth_mass_mg, mouse_data$mass_g)\n\n\n    Pearson's product-moment correlation\n\ndata:  mouse_data$helminth_mass_mg and mouse_data$mass_g\nt = -10.941, df = 37, p-value = 3.741e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9323983 -0.7711347\nsample estimates:\n       cor \n-0.8740017 \n\n\nHere we get an effect size of -0.874 (correlation coefficient), and a p-value of 3.741e-13, which means 3.741 x 10-13, or &lt;&lt;&lt;0.001. This means there is a strong negative relationship observed between mouse mass and helminth mass, and we have very strong evidence for it.\n\n\n18.2.2 Linear Regression\nIf we want to infer cause and effect we can use linear regression (see 9.1.3.2 Cause and Effect). Let’s say we want to know if the number of insects at a site is predictive of mammal mass at a site. First let’s join the two data frames like wed did in the last chapter:\n\n## join our data\nmammals_insects &lt;- insect_counts %&gt;%\n  select(total_insects, site) %&gt;%\n  left_join(fake_mammals, by = \"site\")\n\n## regress mammal mass on total insects with lm function\n## this time I'm specifying the data frame with the data argument\n## then I don't have to write it twice\nmass_model &lt;- lm(mass_g ~ total_insects, data = mammals_insects)\n\n## look at the ouput with summary again\nsummary(mass_model)\n\n\nCall:\nlm(formula = mass_g ~ total_insects, data = mammals_insects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6018 -1.5936  0.0221  2.0487  4.3982 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.15860    1.09484  18.412   &lt;2e-16 ***\ntotal_insects  0.02808    0.01184   2.372   0.0219 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.549 on 46 degrees of freedom\nMultiple R-squared:  0.109, Adjusted R-squared:  0.08959 \nF-statistic: 5.625 on 1 and 46 DF,  p-value: 0.02195\n\n\nIf we look at the coefficient table, we can see that the total insect term has an estimate of 0.028, which is our effect size. For every added insect to a plot, the expected average mass of the mammal community goes up by 0.028g. Connected to that effect size is a p-value of 0.0219, which means we have strong evidence for the relationship.\n\n\n18.2.3 Binomial Regression\nIf your response variable is binary (presence absence), you can use a binomial regression with the glm() function. Let’s test if mammal mass effects the probability of having ticks attached (tick_count).\n\n## first make a presence absence variable for ticks\nfake_mammals$tick_presence &lt;- as.numeric(fake_mammals$tick_count &gt; 0)\n\n## now do the regression, with the binomial \"family\"\ntick_pres_model &lt;- glm(tick_presence ~ mass_g, data = fake_mammals, family = \"binomial\")\n\n## check it\nsummary(tick_pres_model)\n\n\nCall:\nglm(formula = tick_presence ~ mass_g, family = \"binomial\", data = fake_mammals)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -22.3584     6.7603  -3.307 0.000942 ***\nmass_g        0.9857     0.2950   3.341 0.000835 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 54.040  on 38  degrees of freedom\nResidual deviance: 28.181  on 37  degrees of freedom\n  (9 observations deleted due to missingness)\nAIC: 32.181\n\nNumber of Fisher Scoring iterations: 5\n\n\nIf we look at this like we looked at the linear regression, the mass_g term has a very small p-value meaning strong evidence for a relationship. It also has an effect size of 0.9857, meaning that the chance of having a tick increases with body mass. However, the units are in log odds, which are hard to interpret. The reason for this is some stats theory that is beyond the scope of this book/site.\n\n\n18.2.4 Poisson / Negative Binomial Regression\nunder construction….",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analyzing Data</span>"
    ]
  },
  {
    "objectID": "r_analyze.html#multivariate-analysis",
    "href": "r_analyze.html#multivariate-analysis",
    "title": "18  Analyzing Data",
    "section": "18.3 Multivariate Analysis",
    "text": "18.3 Multivariate Analysis\nYou can of course use multiple explanatory variables in your analyses. For example, when we regressed mammal mass on insect count, we ignored mammal species. We could include it like so:\n\n## multiple regression\nmulti_mod &lt;- lm(mass_g ~ total_insects + species, data = mammals_insects)\n\nsummary(multi_mod)\n\n\nCall:\nlm(formula = mass_g ~ total_insects + species, data = mammals_insects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1418 -1.1856  0.1335  1.8338  5.1527 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               19.10561    1.36855  13.960  &lt; 2e-16 ***\ntotal_insects              0.03490    0.01272   2.744  0.00875 ** \nspeciesMeadow vole        -0.29446    1.08017  -0.273  0.78644    \nspeciesWhite-footed mouse  1.02943    0.86932   1.184  0.24270    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.545 on 44 degrees of freedom\nMultiple R-squared:  0.1505,    Adjusted R-squared:  0.09254 \nF-statistic: 2.598 on 3 and 44 DF,  p-value: 0.06421\n\n\nNow we have multiple terms, and since species is categorical, the effect sizes and p-values are based on comparisons to a reference level (deer mouse in this case because it is first alphabetically).\nWe can look at the overall significance of species by running an ANOVA with aov, and summarizing the output:\n\n## we can use the model object in our aov function to save time, it will take the formula\nsummary(aov(multi_mod))\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntotal_insects  1  36.55   36.55   5.643 0.0219 *\nspecies        2  13.93    6.96   1.075 0.3501  \nResiduals     44 285.00    6.48                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooks like there is not much evidence of an effect of species.\nYou may be confused a bit by this code, but essentially t.tests, ANOVAs, and regressions are all “linear models”, and we are specifying them and looking at them differently with different R functions. To learn more, I recommend taking stats classes!\nFinally, you may be wondering which variable to include in your analyses. Model selection is another thing you would learn in stats, but the tl;dr could be: What is your question? Use those variables.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Analyzing Data</span>"
    ]
  },
  {
    "objectID": "r_visualize.html",
    "href": "r_visualize.html",
    "title": "19  Visualizing Data",
    "section": "",
    "text": "19.1 General Notes on Data Visualization\nThere are a few things to keep in mind in general when creating figures, even outside of R:\nUsually, figures should stand alone. This means that your figure can speak for itself, even without a caption. This means that axes and legends are clearly labelled, and trends are emphasized. It can also be helpful to annotate statistical output onto plots themselves.\nWhen you can, show your actual data, instead of summary stats. Generally, when it’s not too messy, seeing all the data points is more informative to the audience. For example, you could plot a comparison of means with a point for each mean, but you could show more if you plot every point behind those means.\nFinally, remember accessibility. Make color schemes appropriate for color-blindness, and make text large.\nNote: for simpler code demonstration purposes, the figures that follow will not always necessarily meet these criteria.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "r_visualize.html#the-tidyverses-ggplot2",
    "href": "r_visualize.html#the-tidyverses-ggplot2",
    "title": "19  Visualizing Data",
    "section": "19.2 The tidyverse’s ggplot2",
    "text": "19.2 The tidyverse’s ggplot2\nWhen it comes to visualizing things in R, there are many methods. You can use the base R functions for plotting (plot, hist, lines, etc.), but I’m not super adept with them. Instead I’ll be walking you through using ggplot2, a package in the tidyverse family that is incredibly popular for data visualization. There is a special syntax that may take some getting used to though.\nEssentially, you create a ggplot “object” (which is another special type of list with unique attributes), and then you pipe it through a series of ggplot functions to add components, themes, labels, etc. However, ggplot2 is older than the %&gt;% pipe we have used, so it uses an old and deprecated pipe operator: +. R automatically knows to interpret + differently with ggplot objects and functions.\nHere is an example of code creating a ggplot figure:\n\n## first create the ggplot object\n## you need to specify your data in the data argument\n## then there is a special set of arguments called aesthetic arguments\n## (bound by the aes() sub-function)\n## these specify what variables will inform aesthetics of your figure\n## (e.g., axes, color, fills, sizes, etc.)\nggplot(data = your_data, aes(x = variable1, y = variable2, color = variable3)) +\n  geom_point(size = 2) + ## then you add geometry, this \"geom\" is for a scatterplot\n  labs(x = \"Variable 1\") + ## then you can add other things like labels\n  scale_color_manual(values = c(\"red\", \"blue\")) + ## or specify scales\n  theme(axis.text = element_text(size = 12)) ## finally you can modify parts of the theme, like fonts\n\nIt may seem complicated at first, but if you start small and work yourself up, you’ll be chaining together code to draw beautiful figures in no time!",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "r_visualize.html#figure-types",
    "href": "r_visualize.html#figure-types",
    "title": "19  Visualizing Data",
    "section": "19.3 Figure Types",
    "text": "19.3 Figure Types\nNow we’ll go over how to make some common figure types, based on your analyses.\n\n19.3.1 One Variable: Continuous\nIf you want to show the distribution of a single variable, you could use a histogram or a density plot.\nFor demonstration, let’s make a plots of white-footed mouse masses.\n\n## create a data frame of only white-footed mice\nwf_mice &lt;- filter(fake_mammals, species == \"White-footed mouse\")\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThere, a simple histogram. Now let’s play with how it looks:\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  ## give a wider binwidth to the histogram, and make it grey bars with black outlines\n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") + \n  labs(x = \"White-footed Mouse Mass (g)\", y = \"Count\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\n\n\n\n\nCool! We could also look at this as a density plot! This will give more of a smooth line\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  geom_density() + ## create density plot\n  labs(x = \"White-footed Mouse Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\n\n\n\n\n\n\n19.3.2 One Variable: Categorical\nIf you want to show how many observations are in each category, you can use a bar plot.\nIn this demo, let’s make a bar plot of how many of each mammal species were caught.\n\n## specify x as species\nggplot(data = fake_mammals, aes(x = species)) +\n  geom_bar() ## make bar plot\n\n\n\n\n\n\n\n\nThe geom_bar function will count up all the observations of each species level to inform its bars. Thus, it is assuming you are giving it long data. Another closely related function is geom_col, which just makes a bar as tall as a number value in the data. For example, let’s make a bar plot of how many insect were caught at each site.\n\n## need to specify two variables this time, one for the category, one for the count value\nggplot(data = insect_counts, aes(x = site, y = total_insects)) +\n  geom_col()\n\n\n\n\n\n\n\n\nAs you can see, your data format will determine whether you should use geom_col or geom_bar. Note: bar plots are generally only best-suited for counts among categories, when you’re dealing with measured variables, there are better options below.\n\n\n19.3.3 Two Variables: Both Continuous\nIf you are showing the relationship between two continuous variables, scatterplots with or without lines are usually the best way to go.\nLet’s try it out with the mammal data on body mass and helminth mass in mice:\n\n## filter for mouse data\nmouse_data &lt;- filter(fake_mammals, species %in% c(\"White-footed mouse\", \"Deer mouse\"))\n\n## create ggplot with your two continuous variables as x and y\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg)) +\n  geom_point() ## create scatterplot\n\n\n\n\n\n\n\n## with trendline\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg)) +\n  geom_point() +  ## create scatterplot\n  ## create a trendline; method = \"lm\" makes it a straight line, se specifys whether there are error regions shaded\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n19.3.4 Two Variables: One Continuous, One Categorical\nBelieve it or not, when one of your variables is categorical, a scatterplot is still appropriate. Why not a bar plot? Because scatterplots show all of your data!\nLet’s demonstrate with the mammal data by comparing helminth mass among species.\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(width = 0.1, height = 0) ## create points that are \"jittered\" a bit along the x axis\n\n\n\n\n\n\n\n\nIn this plot, we use geom_jitter to make the point spread a bit around each categorical X value so that you can see them better (but we specify height = 0 so as not to mess with the mass information). Instead of a mean helminth mass given by a bar plot, we can see the spread of each set of datapoints, including outliers or lack thereof. Still it’s often nice to add some structure to these plots, which can be geom_boxplot or geom_violin (among others). Here is an example:\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(width = 0.1, height = 0) + ## create points that are \"jittered\" a bit along the x axis\n  geom_boxplot(alpha = 0.2) ## create boxplot at 20% transparency with alpha\n\n\n\n\n\n\n\n\nWe could also make this plot even clearer by adding color:\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(aes(color = species), width = 0.1, height = 0) + ## you can put aes() inside geoms\n  geom_boxplot(aes(fill = species), alpha = 0.2) +\n  labs(x = \"Species\", y = \"Helminth Mass (mg)\") +\n  theme_bw() +\n  theme(legend.position = \"none\") ## legend is redundant here, so we can hide it\n\n\n\n\n\n\n\n\n\n\n19.3.5 Non-Axis Variables\nYou can also use other aesthetics to represent variables in your data. For example, you could use color to show the density plots of mammal masses among species. And you can modify the colors with scale functions:\n\n## make a ggplot, use wf_mice data, specify mass as the x variable and species as color\nggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density() + ## create density plot\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) + ## set my own colors\n  labs(x = \"Mammal Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\n\n\n\n\nSimilarly, you can add a third variable to a two variable figure. Take the helminth mass by mammal body mass figure from above:\n\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg, color = species)) +\n  geom_point() +  ## create scatterplot\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nColor isn’t the only way to show variables outside of axes, you can also use point shape, size, linetype, etc. In addition, you can split data among plot panels or “facets”, with facet_wrap() or facet_grid().\nLet’s demonstrate with the long insect data, showing the insect communities for each site:\n\n## specify order as y variable to show labels better\nggplot(data = long_insects, aes(y = order, x = count, fill = site_type)) +\n  geom_col() +\n  facet_wrap(vars(site), nrow = 2) + ## specify site variable, two rows to separate habitats\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n19.3.6 Colorblind Safe Colors\nggplot2 has colorblind-safe color schemes available from the tidyverse-related package viridis.\nFor example:\n\n## make a ggplot, use wf_mice data, specify mass as the x variable and species as color\nggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density(linewidth = 1) + ## create density plot, wider lines\n  scale_color_viridis_d() + ## set viridis discrete colors\n  labs(x = \"Mammal Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\n\n\n\n\nSee the following link for more info:\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "r_visualize.html#saving-figures",
    "href": "r_visualize.html#saving-figures",
    "title": "19  Visualizing Data",
    "section": "19.4 Saving Figures",
    "text": "19.4 Saving Figures\nMuch like your data summaries, you likely will want to save them. You can save figures from the RStudio user interface, i.e., the “Export” button in the “Plots” pane. With the Export button, you can save as an image, and specify the dimensions. This will generally work fine for sharing figures via email or for slide presentations.\nHowever, you can also use the “ggsave” function for figures made with ggplot, which can be useful for creating higher quality images for poster printing. Here is an example:\n\n## first, you need to save your figure as an object in your R environment\n## let's do this with the last plot we made\nmammal_plot &lt;- \n  ggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density(linewidth = 1) + ## create density plot, wider lines\n  scale_color_viridis_d() + ## set viridis discrete colors\n  labs(x = \"Mammal Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n## then you can save it to your project workspace!\n\n## first let's generate a timestamp like we did for summary data\ntimestamp &lt;- format(Sys.time(), format = \"%Y%m%d_%H%M%S\")\n\n## and now use ggsave\n## filename is the first argument, followed by the plot to save\n## here I manually type \".tiff\" in my paste function \n## to specify a high quality image filetype\nggsave(paste(\"mammal_figure_\", timestamp, \".tiff\", sep = \"\"), \n       plot = mammal_plot)\n\nThe ggsave function also features arguments to specify the dimensions of the image in inches, centimeters, pixels, etc. This allows you to make larger figures for posters (12x8 inches often works). However, larger images will often feature tiny text if you don’t specify the axis labels to be larger fonts. You can specify text font size with the “theme” function when drawing a ggplot. For example:\n\nggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density(linewidth = 1) + ## create density plot, wider lines\n  scale_color_viridis_d() + ## set viridis discrete colors\n  labs(x = \"Mammal Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() + ## my favorite simple theme\n  theme(axis.title = element_text(size = 24)) ## enlarge axis titles\n\n\n\n\n\n\n\n\nThere are tons of theme elements that you can modify with the theme function, and you generally have to specify them with “element” wrappers, such as “element_text” above. Do note that if you use a built-in theme like “theme_bw” as I do above, you need to add the “theme” function after the built-in theme to customize further.\nAt first it will take some trial and error to figure out the right combinations of font sizes in the ggplot theme and image sizes in ggsave, but eventually you will likely get a feel for it.",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "r_visualize.html#further-reading",
    "href": "r_visualize.html#further-reading",
    "title": "19  Visualizing Data",
    "section": "19.5 Further Reading",
    "text": "19.5 Further Reading\nWe have only scratched the surface of what ggplot2 can do! We barely discussed how to edit theme elements, nor did we spend much time on customizing scales.\nggplot2 has an excellent reference website which you can find here:\nhttps://ggplot2.tidyverse.org/reference/index.html\nWith it you can learn all the ins and outs!",
    "crumbs": [
      "R Programming",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Visualizing Data</span>"
    ]
  },
  {
    "objectID": "appendix_a.html",
    "href": "appendix_a.html",
    "title": "20  Appendix A: More R Resources",
    "section": "",
    "text": "Here is a collection of links to other useful resources for learning R!\nEcology-themed tutorial:\nhttps://datacarpentry.org/R-ecology-lesson/index.html\nBasic / Base R Materials\nOfficial R manuals:\nhttps://cran.r-project.org/manuals.html\nCookbook for R (features lots of “recipes” for common tasks)\nhttp://www.cookbook-r.com/\nPrimers and Cheatsheets (“tidyverse”-based)\nRStudio primers\nhttps://rstudio.cloud/learn/primers\nTidyverse cheatsheets\nhttps://www.rstudio.com/resources/cheatsheets/\nFull Books and Courses\nHadley Wickham’s “R for Data Science”\nhttps://r4ds.had.co.nz/index.html\nHadley Wickham’s “Advanced R”\nhttps://adv-r.hadley.nz/\nBook for “ggplot2” package\nhttps://ggplot2-book.org/\nJenny Bryan’s STAT 545 course\nhttp://stat545.com/\nPackage Function Reference Sites\nggplot2 (data visualization)\nhttps://ggplot2.tidyverse.org/index.html\nsf (spatial analysis)\nhttps://r-spatial.github.io/sf/index.html\nMiscellaneous Resources\nOn Style:\nhttp://adv-r.had.co.nz/Style.html\nOn Reproducibility:\nhttps://reproducible-analysis-workshop.readthedocs.io/en/latest/\nhttps://swcarpentry.github.io/r-novice-gapminder/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Appendix A: More R Resources</span>"
    ]
  }
]