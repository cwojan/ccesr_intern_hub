[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCESR Intern Hub",
    "section": "",
    "text": "Welcome!\nThis website / HTML book is intended to collect resources for Cedar Creek summer interns doing independent research projects, and present those resources in an easily accessible way.\nFor now, the focus is primarily on data analysis and using the R programming language.\nMuch of the content featured is adapted from the work of past CCESR Fellows, including Mariana Cardenas and Bea Baselga.\nThis site is structured in different parts, which can be read in any order you choose, depending on your needs / what you already know. Currently, the first part goes over data analysis in general, the second part describes R-related software and workflows, and the third part is intended to give a primer in R coding."
  },
  {
    "objectID": "da_glance.html",
    "href": "da_glance.html",
    "title": "1  Data Analysis at a Glance",
    "section": "",
    "text": "Analyzing your data is usually about transforming long spreadsheets into a form that is relevant to your question/s, and oftentimes including an appropriate statistical approach for inference.\nYou might use descriptive statistics, which is simply describing what you observed without presenting every data point, and instead a summary of those data. This can often be helpful in providing a frame of reference to your dataset before looking deeper at trends and comparisons. Alternatively, sometimes descriptive statistics are the main goal - like in surveys of populations and communities (e.g., what is the population size of a certain grass of interest in an old field?). Descriptive statistics include things like the mean and variance, but can also include more niche measures like dispersion.\nYou could also use inferential statistics, which is more about using math or simulation techniques to infer some conclusion form the shape of your data. This is directly relevant to when you have an ecological question about cause and effect, associations among variables, comparisons among categories, etc. The results of inferential statistics provide a starting point from which to interpret/discuss an answer to your question. Examples include t-tests and linear regression.\nWhen using both of these types of statistics, you should be mindful of data types, which are the form that variables take. For example, the height of a tree is number, but the species of a tree is a category. This contrast is obvious, but there are subtle differences that can be important for how you describe, assess, and plot your data."
  },
  {
    "objectID": "da_data.html#numeric-data",
    "href": "da_data.html#numeric-data",
    "title": "2  Data Types",
    "section": "2.1 Numeric Data",
    "text": "2.1 Numeric Data\nAny data that can be described with numbers or have quantifiable relationships between values is numeric. But! There are multiple types of numeric data. The most important distinction is discrete vs continuous.\nDiscrete numeric data is data where not every value is possible, but you can still quantify specific differences among the possible values - the major exmaple being integer values (1, 2, 3, the rest). Most programming languages will refer to this type as integer or int. Examples might include number of ants on a log.\nContinuous numeric data is data where every valuable is possible! So this is basically all real numbers, including decimals (1.0, 1.1, etc.). Many programming languages will refer to this as simply numeric data, but lower level languages might use “float” or “double”. Examples might include th biomass of ants on a log. Note: measures that consist of very large integer values are approximately continuous.\nOther things to consider with numeric data is whether the scale of measurement is bound by any values. For example, the number of or biomass of ants on a log cannot be less than zero. In addition, percentages and proportions are bound by 0 and 100 and 0 and 1 respectively. These limitations can lead to special considerations when performing inferential statistics."
  },
  {
    "objectID": "da_data.html#categorical-data",
    "href": "da_data.html#categorical-data",
    "title": "2  Data Types",
    "section": "2.2 Categorical Data",
    "text": "2.2 Categorical Data\nAny data for which the values have no specifically quantitative difference among them is categorical. Again there is one majorly important distinction: nominal vs ordinal.\nNominal data is data where categories have no ranking or order, like the species of ants on a log.\nOrdinal data is data where categories have some order, like your top 5 favorite breakfast cereals. But wait! You may be thinking - “isn’t this quantitative?” Well yes and no. The difference between ordinal data and discrete numeric data is that you can’t really quantify the exact difference between ordinal data values. Say there is a go-kart race between Mario, Luigi, and Peach. The place that each finished would be ordinal, e.g., Peach got 1st and Luigi 2nd, but you wouldn’t be able to say how much faster Peach was than Luigi. The time it took for Peach and Luigi each to finish the race would be a numeric variable, and there would be a specific value difference between them."
  },
  {
    "objectID": "da_describe.html#centrality",
    "href": "da_describe.html#centrality",
    "title": "3  Descriptive Statistics",
    "section": "3.1 Centrality",
    "text": "3.1 Centrality\nYou’ll often want to describe the central tendency of your data - around where are the values centered?\nMean - the average of the values, or the sum of all values divided by the number of observations\nMedian - the value at which half of the observations are greater, and the other half are less\nMode - the most commonly observe value\nUsually, the mean is a a perfectly adequate descriptor. You can use it on continuous numeric data, discrete numeric data (though the mean value will often be unrealistic), or even ordinal rankings.\nWhen might you prefer to use the median over the mean?\nWhen the data is skewed such that there are many small values and a few big values, the mean might be inflated by those large values, and thus overestimate the central tendency in some contexts.\nWhen data is roughly normally distributed, the mean and median are roughly the same:\n\n\n\n\n\nBut when data are skewed, the median may be a better estimate of the central tendency:"
  },
  {
    "objectID": "da_describe.html#spread",
    "href": "da_describe.html#spread",
    "title": "3  Descriptive Statistics",
    "section": "3.2 Spread",
    "text": "3.2 Spread\nYou also might be interested in how varied your data is, how much it deviates from the central tendency. This can be done with the following:\nVariance - how variable is the data? Measured as the average squared difference between observations and the mean:\n\\[\nVariance = \\frac{\\sum (Observation_i - Mean)^2}{Number of Observations}\n\\]\n(\\(\\sum\\) means “sum of”)\nThe differences are squared to get rid of negative differences, because other wise everything would cancel out and our variance would be zero!\nStandard Deviation - the square root of the variance. This is useful because it is in the same units as the original measurements!"
  },
  {
    "objectID": "da_describe.html#other-descriptors",
    "href": "da_describe.html#other-descriptors",
    "title": "3  Descriptive Statistics",
    "section": "3.3 Other Descriptors",
    "text": "3.3 Other Descriptors\nAnother descriptor that may prove useful is the dispersion, or the variance divided by the mean. This provides an estimate of how skewed the data is - for example, the first plot above has very low dispersion, while the second plot has high dispersion."
  },
  {
    "objectID": "da_describe.html#ecological-community-descriptors",
    "href": "da_describe.html#ecological-community-descriptors",
    "title": "3  Descriptive Statistics",
    "section": "3.4 Ecological Community Descriptors",
    "text": "3.4 Ecological Community Descriptors\nMany of you are interested in describing the species composition of of community. Here’s a few common descriptors:\nSpecies Richness - this is just the number of different species present.\nSpecies Diversity - this is an index that takes into account the richness as well as the relative abundances of each species. E.g. Shannon’s Diversity Index, where higher numbers mean more species more evenly distributed.\nSpecies Evenness - this is an index that estimates specifically how evenly distributed species abundances are. E.g., Pielou’s Evenness, which ranges from 0 to 1, with 1 meaning that each species has equal numbers.\nNote: these measures can apply to any taxonomic distinction, e.g., family richness, order diversity, etc."
  },
  {
    "objectID": "da_infer.html#classic-frequentist-tests",
    "href": "da_infer.html#classic-frequentist-tests",
    "title": "4  Inferential Statistics",
    "section": "4.1 Classic Frequentist Tests",
    "text": "4.1 Classic Frequentist Tests\nNow let’s go over some statistical tests! For this section, it can be useful to remind ourselves of the variables involved in a research question:\nIndependent / Explanatory / Predictor Variable: this is either what you are manipulating in an experiment or what your study is designed to capture variation in (e.g., C02 at BioCON, species richness at BigBio).\nDependent / Response Variable: these are what you measure or observe throughout your study, generally hypothesizing that they will differ among the levels of your independent variable (e.g., aboveground biomass in BioCON or BigBio).\n\n4.1.1 Assumptions\nWe should mention what these tests generally assume about your data.\nFirst, they assume that your data are independent. This just means that no two observations of your data are more related to eachother in a way that isn’t accounted for by a variable. Say you were comparing mean tree height between two forests - individual tree heights in the same forest would be independent, but two measures of the same tree on different days would be non-independent.\nSecond, they assume that the errors are normally distributed. This is a bit more confusing without a statistical background. An example may be illustrative - in the tree height example above, we assume that the individual tree height are normally distributed about the mean. Without getting too much into the weeds, if you collect enough data (i.e., 30+ observations), these errors will likely be approximately normally distributed. However, things get divcey when we deal with data that is not continuous like tree height, for example, discrete count data - more on that below.\nThird, they assume homogeneity of variance. This is another complicated one, but it mean that the variance of the errors doesn’t change with the independent variable. In the tree example, we are assuming that the variance of the differences between observed tree heights and the forest mean does not change between forests.\nData that break the first assumption are difficult to deal with outside of accounting for the non-independence factor (which can severely reduce the size of your sample), but failing to meet the second or third assumptions generally leads to transforming data or using alternative tests.\n\n\n4.1.2 Categorical Predictor/s, Numeric Response\n\n4.1.2.1 Two Predictor Categories\nWhen you are comparing numeric values from two groups, you can use a t-test to compare their means. T-tests can be paired when each observation in one group is specifically linked to an observation in the other group (e.g., masses of sibling plants in separate treatments) which can be more powerful. When the variance of values in each group changes, you can do a t-test with unequal variance.\nThe effect size here is the difference between means.\n\n\n4.1.2.2 More Than Two Predictor Categories\nIf you have more than two groups/categories, you can use a Analysis of Variance or ANOVA. This will tell you if the means of each group are equivalent, or if there is at least one inequality. You can test for pairwise comparisons among the groups with Tukey’ test. If you have multiple categorical predictors, you can do two-way or three-way ANOVAs. Tests with more than three categorical predictor variables are uncommon and harder to interpret.\nThe effect sizes are the pairwise difference in means.\n\n\n4.1.2.3 Ordinal Predictors\nWhen your predictor variable is ordinal, the quick and easy way to analyze it would be to convert the predictor to a numeric integer data type and proceed from there. However this is imprecise…\nThis section is under construction\n\n\n\n4.1.3 Numeric Predictor/s, Numeric Response\n\n4.1.3.1 Simple Association\nWhen all you are interested in is whether two numeric variables are related to one another, not cause and effect, you can do a correlation test. Pearson’s correlation is generally applicable for continuous data. Spearman’s correlation is good for when you are dealing with data with non-normal distributions, like count data (it also works for ordinal data!).\nThe effect size here will be a correlation coefficient ranging from -1 to 1, with -1 means an inverse relationship, 0 means no relationship, and 1 mean a direct positive relationship.\n\n\n4.1.3.2 Cause and Effect\nWhen you are suppose a causal relationship between numeric variables, you can use a linear regression. This will use linear algebra or maximum likelihod estimation (don’t worry about it) to find the best fit line that describes the relationship between two variables; where the sum of the squared distances from the observations to the line is minimized. You can also include multiple predictor variables to perform multiple linear regression AKA multivariate linear regression.\nWhen your response variable is count data, the assumptions of simple linear regression are usually unmet, so you can use generalized forms like a Poisson regression or a Negative Binomial Regression.\nThe effect sizes here are the parameter coefficients, i.e., how much does the response change for on unit increase in the predictor? Note: these are not straightforward for Poisson and negative binomial regression, so ask your mentor.\n\n\n\n4.1.4 Numeric Predictor/s, Categorical Response\n\n4.1.4.1 Binary Response\nWhen your categorical response is only two categories (e.g., presence or absence), you can use a binomial regression AKA logistic regression. This works similarly to linear regression, but the effect sizes are measured in log odds, which is difficult to interpret, but can be transformed to estimating how the probability of one category value over the other increases with a variable.\n\n\n4.1.4.2 Multiple Response Categories\nMultinomial regression (under construction)\n\n\n\n4.1.5 Categorical Predictor/s, Categorical Response\nChi-square test (under construction)"
  },
  {
    "objectID": "da_infer.html#bootstrapping",
    "href": "da_infer.html#bootstrapping",
    "title": "4  Inferential Statistics",
    "section": "4.2 Bootstrapping",
    "text": "4.2 Bootstrapping\nOne alternative to these classic tests has no assumptions: bootstrapping. Essentially, it involves using the sampled data to simulate more samples, and compare your observations to those simulations.\nEmpirical Bootstrapping is where you take your actual observations and shuffle which value is associated with which observation. For example, you could take measurements of tree heights from two forests, and randomly assign forest ID to each measurement.\nParametric Bootstrapping is where you summarize your observed data and use it to generat simulated data. For example, you could calculate the mean and variance of tree heights in two forests and then generate simulated forests of trees through random pulls from a normal distribution with the appropriate mean and variance.\nWith both approaches, you simulate a large number of simulated datasets (1000+), and then calculate whatever you are interested in for each of those simulations, and compare the calculation from the observed data to the distribution of simulated values. For example, if you empirically bootstrap the two forests of tree heights 1000 times, and then calculate difference in means for each you will have 1000 mean difference values. The proportion of those simulated values that are equal to or more extreme than your observed mean difference is your p-value!"
  },
  {
    "objectID": "rc_r.html#r-the-language",
    "href": "rc_r.html#r-the-language",
    "title": "5  R Itself",
    "section": "5.1 R, the Language",
    "text": "5.1 R, the Language\nR is a programming language designed for statistical computing, and is often the language of choice for scientists. R is also used for data science in some business, tech, and health contexts ( but many prefer Python in those areas).\nAs a programming language it is essentially an expandable collection of functions with syntax to perform tasks, and it could be written in any text editor. However, in order for your computer to interpret the language, it needs some software."
  },
  {
    "objectID": "rc_r.html#r-the-software",
    "href": "rc_r.html#r-the-software",
    "title": "5  R Itself",
    "section": "5.2 R, the Software",
    "text": "5.2 R, the Software\nThe R application allows you to run R code on your computer, and comes with a basic “console” window where code is run and output is printed, as well as a basic script editor where you can write code to run.\nYou can download the application from here:\nhttps://cran.r-project.org/\nIf you are asked to select a mirror, simply select the nearest one (I believe Iowa State should work).\nIf you have a Windows machine, it should be fairly straightforward to simply download and install the “base” R from the link.\nIf you have a Mac, you will want to select the .pkg file that matches your processor type: x-86 for Intel processors (mostly Macs pre-2020), arm64 for Macs with the M1 or M2 chip (most Macs post-2020).\nIf you are using Linux, you know more than me."
  },
  {
    "objectID": "rc_r.html#r-packages",
    "href": "rc_r.html#r-packages",
    "title": "5  R Itself",
    "section": "5.3 R Packages",
    "text": "5.3 R Packages\nAs mentioned above, R is expandable. You can add more functionality to R by installing packages. Packages contain more options of code to use to process and analyze data, and also do many other things.\nPackages can be installed through writing R code, or by clicking some buttons in RStudio. Then they will live in a directory that was built when you installed R for auxiliary packages.\nWe will discuss more about installing packages in the R coding section."
  },
  {
    "objectID": "rc_rstudio.html#rstudio-at-a-glance",
    "href": "rc_rstudio.html#rstudio-at-a-glance",
    "title": "6  R Studio",
    "section": "6.1 RStudio at a Glance",
    "text": "6.1 RStudio at a Glance\nIf you open up RStudio, you will see something like this:\n\n1- Script Editor: Here is where you will write code! You can create an R script (a text document to save code in) with the file tab, and write what you need in the resulting script. It is highly recommended to use scripts, because then you can save your code for later, and troubleshoot errors easier. From this window, you can highlight code and run it with the “Run” button on top, or with Ctrl + Enter / Cmd +Enter.\n2- R Console: Here is where the action happens - code will run here, and text output, warnings and messages will be displayed. You can also type code into the console, but that is only recommended for installing packages, entering credentials, rendering documents, and things of that nature. Don’t type your data processing or analysis code into the console, use a script instead! There’s also a terminal tab if you ever need to perform shell commands.\n3- Environment and History: Here you can find a list of the variables and data you have loaded into your “workspace” or “environment” in the Environment tab. These are objects you can do stuff with with code. You can also click the History tab to see the code you have run thus far.\n4- Files and Plots: Here is where any figures you draw will pop up (and you can save them from here as well). There is also a Files tab that allows you to navigate through your file directory (helpful with projects, described below). The Packages tab shows which packages you have installed and loaded (you can also click “Install” at top to easily install new ones!). Finally, the help tab is where you can search for the documentation on any R function."
  },
  {
    "objectID": "rc_rstudio.html#sec-rprojects",
    "href": "rc_rstudio.html#sec-rprojects",
    "title": "6  R Studio",
    "section": "6.2 R Projects",
    "text": "6.2 R Projects\nIt is highly recommended to use R Projects when working with RStudio. Projects are essentially just subdirectories in your file folders, but they come with a special .Rproj file that RStudio can read and use. This helps you organize your work, and makes your code more easily portable.\nYou can create a new projects from the File tab at upper left, or in the project dropdown menu at upper right. You can just create one in a new directory. Then you can select a name and where you want to save it.\nThere are many different types of projects - this book/website is one!\nIf you want to backup your work with version control or collaborate with others using git and GitHub, you will need to use projects. (Well, technically you don’t need to, but you’d be doing many things manually)."
  },
  {
    "objectID": "rc_git.html",
    "href": "rc_git.html",
    "title": "7  Optional: Git and Github",
    "section": "",
    "text": "If you are interested in:\n\nBacking up your code using a version control system that allows you to roll back changes and monitor incremental progress\nand/or\nSharing your code and collaborating with others\n\nYou may like to try using git (a program for your computer) and GitHub (a website that hosts code projects).\nWe won’t go into detail here, but Jenny Bryan’s excellent introduction and tutorial on the topic can be found here:\nhttps://happygitwithr.com/"
  },
  {
    "objectID": "r_basics.html#intro",
    "href": "r_basics.html#intro",
    "title": "8  The Basics",
    "section": "8.1 Intro",
    "text": "8.1 Intro\nI’m sure those of you reading this come from a wide variety of backgrounds regarding computer programming - some of you may be very familiar with it, others total novices. Some of you may love computing, others might hate it. If you’re apprehensive about learning R, or if you find yourself struggling with it - don’t worry! Scientific computing presents a challenge at some point to everyone who does it. Just remember a few things:\n\nEveryone makes mistakes.\nDon’t be afraid to ask questions!\nDon’t compare yourself to others, compare you today to you yesterday.\nEveryone is constantly learning new things, including those who seem like experts.\n\nThat said, learning a programming language is a little like learning a human language, except there’s a much smaller vocabulary and the grammar is very strict. And where human language has parts of speech like nouns and verbs, R has a certain syntax as well. Some of the main components of the R language are operators, functions, arguments, and data."
  },
  {
    "objectID": "r_basics.html#operators",
    "href": "r_basics.html#operators",
    "title": "8  The Basics",
    "section": "8.2 Operators",
    "text": "8.2 Operators\nOperators are short symbols that tell the computer to do certain simple things. You are already familiar with many operators - the math operators like +, -, *, and /. R at its simplest is a calculator:\n\n## This is block of R code! Anything that starts with # is a comment, and doesn't run.\n\n## adding\n2 + 2\n\n[1] 4\n\n## subtracting\n5 - 4\n\n[1] 1\n\n## multiplying\n3 * 3\n\n[1] 9\n\n## dividing\n6 / 2\n\n[1] 3\n\n\nThere are a couple other math operators too:\n\n## exponentiate with ^\n3^2\n\n[1] 9\n\n## find the remainder with the modulus, %%\n10 %% 3\n\n[1] 1\n\n## perform integer division with %/%\n10 %/% 3\n\n[1] 3\n\n\nBut math operators aren’t the only type! There are also the closely related comparison operators, which will return TRUE or FALSE instead of calculated numbers:\n\n## equals, ==\n2 + 2 == 4\n\n[1] TRUE\n\n## does not equal, !=\n2 + 2 != 4\n\n[1] FALSE\n\n## greater than, &gt;\n5 &gt; 4\n\n[1] TRUE\n\n## less than, &lt;\n5 &lt; 4\n\n[1] FALSE\n\n\nThere are also greater than or equal to (&gt;=) and less than or equal to (&lt;=).\nYou can combine comparisons with logical operators - and (&), or (|), and not (!):\n\n## and: are both true?\n(3 &gt; 2) & (4 &gt; 3) \n\n[1] TRUE\n\n## or: is at least one true?\n(2 == 1) | (4 &lt; 3)\n\n[1] FALSE\n\n## not: is this false?\n!(2 == 1)\n\n[1] TRUE\n\n\nThere are few other important operators, but they will make more sense once we talk about the other parts of R."
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "8  The Basics",
    "section": "8.3 Functions",
    "text": "8.3 Functions\nFunctions are words (though not necessarily real words) or letters that instruct the computer to perform more complicated tasks. They generally are followed by parentheses ().\n\n## here's a function that returns the current date\nSys.Date()\n\n[1] \"2023-07-28\"\n\n## and here is a function that returns the date with the time\nSys.time()\n\n[1] \"2023-07-28 15:25:36 CDT\"\n\n\nNo you may be thinking - “this is pretty basic” and “what are the parentheses for?”, which brings use to arguments!"
  },
  {
    "objectID": "r_basics.html#arguments",
    "href": "r_basics.html#arguments",
    "title": "8  The Basics",
    "section": "8.4 Arguments",
    "text": "8.4 Arguments\nArguments are values or objects that go inside the parentheses of functions to specify what you want the function to do. This is what gives functions their power. Arguments are separated inside a function by commas.\n\n## the sum function can sum many numbers\nsum(1,2,3,4,5)\n\n[1] 15\n\n\nIn the function above, each number is acting as an argument. In this case, the arguments don’t have names. Oftentimes a function’s arguments will be explicitly named, and to specify what you want those arguments to be, you use the = operator.\n\n## this function pulls values randomly from a normal distribution specified in the arguments\n## n specifies how many numbers to return, and mean and sd specify shape of the distribution\nrnorm(n = 10, mean = 5, sd = 1)\n\n [1] 4.665984 4.505987 4.272058 4.556169 5.173812 4.616402 7.648465 4.603959\n [9] 5.838315 5.449805\n\n\nOperators are actually a special type of function that can be used with syntax that is more intuitive for them. You can also use them in the same way as most functions by surrounding them with back ticks, `.\n\n## here we use the + operator in a much more confusing context\n`+`(2, 2)\n\n[1] 4\n\n## it is equivalent to\n2 + 2\n\n[1] 4"
  },
  {
    "objectID": "r_basics.html#data",
    "href": "r_basics.html#data",
    "title": "8  The Basics",
    "section": "8.5 Data",
    "text": "8.5 Data\nWe are using the word data here to broadly encompass values (like the numbers we were using above, both with operators and as arguments), variables (stored values), and data structures (organized collections of values).\n\n8.5.1 Values\nValues are much like the data types we discuss in the data analysis section. In fact, the different types of values R can deal with are called data types as well!\nIn R, values can be numeric, character, or logical (among other, more specific types).\n\n## numeric values are numbers!\n2\n\n[1] 2\n\n2.5\n\n[1] 2.5\n\n## character values are letters, words, phrases (often referred to as \"strings)\n\"a\"\n\n[1] \"a\"\n\n\"apple\"\n\n[1] \"apple\"\n\n\"there is a worm in my apple\"\n\n[1] \"there is a worm in my apple\"\n\n## note: character values or strings must be surrounded by \"\" or '' for R to interpret them as strings\n\n## Logical values are TRUE or FALSE (you've seen these above)\nTRUE\n\n[1] TRUE\n\nFALSE\n\n[1] FALSE\n\n\nThere are other types of values too: missing values (NA and NaN), infinite values (Inf and -Inf), and something that indicates empty (NULL).\n\n\n8.5.2 Variables\nVariables are named values that are stored in the “environment”, or the workspace that R can access to perform its tasks. In order to store a value as a variable, you need to use a special kind of operator called an assignment operator (&lt;- or =). As I mentioned variables have names, which are unquoted text.\n\n## store 2 as a variable called x\nx &lt;- 2\n\n## R returns no output here because you're just storing a value\n## but you can return the value by calling the variable\nx\n\n[1] 2\n\n## store 3 as a variable called y\ny &lt;- 3\n\n## you use variables with operators\nx + y\n\n[1] 5\n\n## store a character value\nstring &lt;- \"hello\"\n\n## math doesn't work on strings\n\nTechnically, you can use = in place of &lt;-. This is why the equals operator is ==. I generally use &lt;- to prevent any confusion between assignment and comparison.\n\n8.5.2.1 Naming Rules\nVariables have rules about how they can be named:\n\nNo special symbols other than _ and .\nYou can’t start with a number or _.\nThey can’t be special words that R interprets differently. You can enter ?Reserved in your console to see a list.\n\n\n\n\n8.5.3 Data Structures\nData structures are collections of values with some sort of organization, and also saved in the environment. Plot twist: the variables above are a the simplest data structure, the scalar, which is just a single value.\nThe next data structure is the vector, which is a collection of values of the same data type. We can store them much like variables.\n\n## we use another operator, :, to create a sequence of integers from 1 to 10\nmy_vector &lt;- 1:5\n\nmy_vector\n\n[1] 1 2 3 4 5\n\n## you can also create vectors with the combine function, c()\nmy_other_vector &lt;- c(\"a\", \"b\", \"c\")\n\nmy_other_vector\n\n[1] \"a\" \"b\" \"c\"\n\n\nThe next data structure is called a list. A list is a collection of values like a vector, but they can be of any data type, or data structure. You can have a list of numeric values and character values, a list of vectors, or even a lists of lists! Every other complex data structure is technically a list with special attributes and/or rules.\n\n## you can create lists with the list function\nmy_list &lt;- list(\"a\", 1, 2:4)\n\nmy_list\n\n[[1]]\n[1] \"a\"\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 2 3 4\n\n## can also use the combine function, but it will default to a vector when data types are the same\nmy_other_list &lt;- c(\"b\", 2)\n\nFinally, the most common special type of list you will use is the data frame. A data frame is a list of vectors that are arranged in a table, much like an excel spreadsheet. Each of the vectors will be named as a column, and all must be the same length. The position of a value in a vector is its row in the data frame.\n\n## we can make a data frame with the data.frame function\nmy_data &lt;- data.frame(letter = c(\"a\",\"b\",\"c\"), # each column has a name\n                      number = c(1, 2, 3),\n                      vowel = c(TRUE, FALSE, FALSE))\n\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c      3 FALSE\n\n\nNext, we will extend these concepts a bit further!"
  },
  {
    "objectID": "r_next.html#packages",
    "href": "r_next.html#packages",
    "title": "9  Next Steps",
    "section": "9.1 Packages",
    "text": "9.1 Packages\nPackages are collections of R functions that people write to make tasks easier. One of the strengths of R is that countless programmers have taken the time to assemble functions of use in their respective fields, and shared them with the world. For example the “vegan” package contains a number of functions geared towards community ecology, like calculating diversity indices. You could calculate a diversity index with just the base R, but it would be more difficult and take longer.\nYou can install packages in at least two ways:\n\nYou can use the following code, with the package names in quotes (this is one of the few times where using the console is recommended, because you only need to install a package once):\n\ninstall.packages(\"PACKAGE NAME HERE\")\n\nOr you can use the packages tab in RStudio. In the lower right panel, there should be a packages tab in between “Plots” and “Help”. Once there, there is an “Install” button. When clicked a window will appear allowing you to search for packages to install.\n\nBut Installing packages does not make them automatically accessible to you. When R boots up, it only loads its base functionality by default, so you have to load any packages that you want to use for a given R session. You can do this with the following code (with the package name not in quotes):\n\nlibrary(PACKAGE NAME)\n\nThe code for loading packages should be saved in your r script, because it will need to be done every time you open R.\nThere is a family of packages that is very popular called the “tidyverse.” The aim of the tidyverse is to make data manipulation and visualization streamlined and efficient. Some people are very opinionated about whether you should use the tidyverse or base R, but in my opinion, it’s mostly silliness. If you only want to dip into R and don’t plan to use it much in the future, you may as well just pick up the specific functions you need to use and not worry about much else. If you’d like to continually use R for data analysis, but don’t plan on getting deep into it, getting a handle on the tidyverse may be a good idea. If you want to really get into R, I would recommend learning how to do things in base R (as well as tidyverse functions).\nYou can install the tidyverse suite with:\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "r_next.html#subsetting",
    "href": "r_next.html#subsetting",
    "title": "9  Next Steps",
    "section": "9.2 Subsetting",
    "text": "9.2 Subsetting\nIn the last section we introduced data structures. Now let’s talk about what you can do with them.\n\n9.2.1 Vectors\nThe individual elements of a vector can be accessed with bracket operators - [ and ]. You can refer to an element by its index, or its numeric place in the sequence of elements (e.g., the 1st, the 10th, etc.). It’s important to not here that R starts counting at 1, while many other programming language start counting at 0 (e.g., Python). This is another thing that people are opinionated about, and if you put your mind to it, you can be too! Anyway, here are some examples:\n\n## let's create a vector of the first five letters of the alphabet\nmy_vector &lt;- c(\"a\",\"b\",\"c\",\"d\",\"e\")\nmy_vector\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n## now let's return the 5th element\nmy_vector[5]\n\n[1] \"e\"\n\n## we can return multiple elements with c()\nmy_vector[c(2,4)]\n\n[1] \"b\" \"d\"\n\n## or as a series with :\nmy_vector[2:4]\n\n[1] \"b\" \"c\" \"d\"\n\n\nYou can also use negative numbers to exclude values from what’s returned:\n\n## lose the last element\nmy_vector[-5]\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n## everything but the last element\nmy_vector[-1:-4]\n\n[1] \"e\"\n\n\n\n\n9.2.2 Lists\nSubsetting vectors is fairly straightforward, but subsetting lists can be tricky. Since lists have multiple levels of organization, they use both the [] operators and the [[]] operators. Single brackets give you the list element, and double brackets give you what the list element contains. Let’s demonstrate:\n\n## create a list \nmy_list &lt;- list(c(\"a\",\"b\",\"c\"), \"d\", \"e\")\nmy_list\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] \"d\"\n\n[[3]]\n[1] \"e\"\n\n## grab the first list element\nmy_list[1]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n## grab what's conatined in the first list element (in this case a vector)\nmy_list[[1]]\n\n[1] \"a\" \"b\" \"c\"\n\n## another example with a scalar\nmy_list[2]\n\n[[1]]\n[1] \"d\"\n\nmy_list[[2]]\n\n[1] \"d\"\n\n## you can also subset what you have subsetted:\nmy_list[[1]][1]\n\n[1] \"a\"\n\n## but if you try subsetting a list element, it won't work the same way\nmy_list[1][1]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n## this is because [] returns the list element as a list of length 1, therefore [1] gives you the same thing again, and [2] would give you a NULL list.\n\nThis distinction can be difficult to understand, but don’t worry! It takes time. The best analogy I’ve seen is from Hadley Wickham here:\nhttps://adv-r.hadley.nz/subsetting.html#subset-single\nYou can think of a list as a train, every list element is a train car, and each has its own contents. Single brackets give you the train car/s, and double brackets gives you what’s inside a single train car. And even a single train car is a train (or a list). Also note:\n\n## you can grab multiple list elements with []; this give a list with two elements\nmy_list[1:2]\n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n[[2]]\n[1] \"d\"\n\n## list elements can be named and indexed by their name as well\nnamed_list &lt;- list(first = 1:3, second = 10)\nnamed_list\n\n$first\n[1] 1 2 3\n\n$second\n[1] 10\n\nnamed_list[\"first\"]\n\n$first\n[1] 1 2 3\n\n\n\n\n9.2.3 Data Frames\nSubsetting data frames is a little easier to get a handle on, you just need to think in two dimensions. When using single brackets to subset data frames, you need to specify the index of the row and the column separately and in that order. You separate each index number by a comma inside the brackets. Check it out:\n\n## create data frame\nmy_data &lt;- data.frame(letter = c(\"a\",\"b\",\"c\"), # each column has a name\n                      number = c(1, 2, 3),\n                      vowel = c(TRUE, FALSE, FALSE))\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c      3 FALSE\n\n## grab the element in the 2nd row, 1st column\nmy_data[2,1]\n\n[1] \"b\"\n\n## you can also grab a whole row or column by leaving onse side of the comma blank\nmy_data[2,]\n\n  letter number vowel\n2      b      2 FALSE\n\nmy_data[,1]\n\n[1] \"a\" \"b\" \"c\"\n\n## (subsetting a row gives you a data frame, subsetting a column gives you a vector)\n\nBut data frames also have named columns! Let’s use that to our advantage. You can specify a column’s name instead of its index in brackets, like for a list, or you can use the $ operator.\n\n## subsetting by name in brackets\nmy_data[,\"vowel\"]\n\n[1]  TRUE FALSE FALSE\n\n## subsetting by name with $ (notice no quotes)\nmy_data$vowel\n\n[1]  TRUE FALSE FALSE\n\n## the downside of $ is that you can't grab more than one column like with brackets\nmy_data[,c(\"letter\", \"vowel\")]\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## subsetting multiple columns gives you a data.frame\n\n## you can use $ with named lists too\nnamed_list$first\n\n[1] 1 2 3\n\n## you can mix subsetting operators if you ever need to\nmy_data$vowel[1]\n\n[1] TRUE\n\nmy_data[1,]$vowel\n\n[1] TRUE\n\n\nYou can also use brackets to select rows by value, not index. You just need to use some comparison operator in a statement that resolves as TRUE or FALSE.\n\n## grab the consonant rows\nmy_data[my_data$vowel == FALSE,]\n\n  letter number vowel\n2      b      2 FALSE\n3      c      3 FALSE\n\n## grab the rows before the third\nmy_data[my_data$number &lt; 3,]\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n\n## you can combine criteria\nmy_data[my_data$number &lt; 3 & my_data$vowel == FALSE,]\n\n  letter number vowel\n2      b      2 FALSE\n\n\nNow the reason that we talked about packages in between data structures and subsetting is because the tidyverse (specifically, the dplyr package) has more functions for subsetting: filter and select. Filter works much like grabbing rows by value, and select works like grabbing columns by name. Let’s look at some examples:\n\n## load the tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## filter for consonants\nfilter(.data = my_data, vowel == FALSE)\n\n  letter number vowel\n1      b      2 FALSE\n2      c      3 FALSE\n\n## select letter related columns\nselect(.data = my_data, letter, vowel)\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## you can also exclude columns\nselect(.data = my_data, !number)\n\n  letter vowel\n1      a  TRUE\n2      b FALSE\n3      c FALSE\n\n## note: selecting a single column will give a data frame, not a vector\nselect(.data = my_data, number)\n\n  number\n1      1\n2      2\n3      3\n\n## another tidyverse/dplyr function, pull, will give just a vector\npull(.data = my_data, number)\n\n[1] 1 2 3\n\n\nAs you can see, filter, select and pull are versatile, consistent and powerful. However, they lack one key ability: assignment. You can use brackets and $s to assign things:\n\n## assign a new value to a data element (NA means missing value)\nmy_data[3,2] &lt;- NA\nmy_data\n\n  letter number vowel\n1      a      1  TRUE\n2      b      2 FALSE\n3      c     NA FALSE\n\n## create a whole new column with $ (vector must be of same length as the number of rows)\nmy_data$new_column &lt;- c(\"some\", \"new\", \"data\")\nmy_data\n\n  letter number vowel new_column\n1      a      1  TRUE       some\n2      b      2 FALSE        new\n3      c     NA FALSE       data"
  },
  {
    "objectID": "r_next.html#optional-flow-control",
    "href": "r_next.html#optional-flow-control",
    "title": "9  Next Steps",
    "section": "9.3 Optional: Flow Control",
    "text": "9.3 Optional: Flow Control"
  },
  {
    "objectID": "r_next.html#optional-writing-functions",
    "href": "r_next.html#optional-writing-functions",
    "title": "9  Next Steps",
    "section": "9.4 Optional: Writing Functions",
    "text": "9.4 Optional: Writing Functions"
  },
  {
    "objectID": "r_import.html#reading-data",
    "href": "r_import.html#reading-data",
    "title": "10  Importing Data",
    "section": "10.1 Reading Data",
    "text": "10.1 Reading Data\nImporting data into R is often referred to as reading data, as that is what the computer is doing, it’s reading the contents of a file (usually a text file). Most ecologists and data scientists work with a text file called a Comma Separated Value file, or csv. This is a small file that’s easy for computers to read where each column is separated by a column, and each row by a new line. You can save excel files as csv from the “Save As…” menu, and you can specify csv as the type when downloading a Google sheet.\n\n10.1.1 From Your Computer\nSo when you have files you want to read locally on your computer, the first thing you need to think about is what’s called the “working directory”. The working directory is the folder on your computer where R will look for files when prompted, and also where it will save output.\nYou can check your current working directory:\n\n## return current working directory\ngetwd()\n\n[1] \"/Users/kit/Documents/UMN/Research/cedar_creek_projects/ccesr_intern_hub\"\n\n\nYou can also set your working directory manually\n\n## change working directory\nsetwd(\"some/different/folder\")\n\nOr, in RStudio, you can click the Session dropdown menu at the top of the window, then “Set Working Directory”, then “Choose Directory.”\nIf you use an R Project (highly recommended), you don’t have to worry as much about this. If you have a project open, the working directory will be automatically set to the folder that contains the .Rproj file that is created when you create a project. See Section 6.2 for more info!\nWhen you’re in an R Project, or have a csv you want in your working directory, you can read it into your environment like so:\n\n## read data\nmy_data &lt;- read.csv(\"the name of your file in quotes\", header = TRUE)\n\nThe read.csv function creates a data frame from the csv you specify, and then the &lt;- assigns it to “my_data.” The “header = TRUE” argument tells R to interpret the first line of the csv as the column names.\n\n\n10.1.2 tidyverse Function\nThe readr package in the tidyverse family also has its own data reading functions.\n\n## load tidyverse\nlibrary(tidyverse)\n\n## read data (assumes header by default)\nmy_data &lt;- read_csv(\"name of your data in quotes\")\n\nThese functions are pretty similar, with one exception: read.csv gives you a data frame, but read_csv gives you a “tibble.” What is a tibble? It’s another special type of list, much like a data frame, but with a few differences. IT was designed to work more consistently with tidyverse functions. One important difference between data frames and tibbles that when you subset an individual column with the brackets ([]), data frames will give you vectors, and tibble will give single column tibbles. This has caused me confusion when writing functions, but you may not run into it.\n\n\n10.1.3 From The Web\nYou can also read files directly from the web. If you have your data in Google sheets, you can create a URL for R to import it directly. Simply go to the File menu, click “Share” and then “Publish to web”. In the box that pops up, you will need to select the file type as “.csv”, not web page. Then save the URL that it gives you!\nFor demonstration, I’ve created a few data sheets that you too can import into R by copying the following code:\n\n## put the url of the data in quotes\nfake_mammals &lt;- read.csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQ9mfx88nM33PC6WpIh3nSxMvkM98nEszw5gpUq7KdqbiCskF8Pqvrl0W2EqNf9rD1JEepb-hSMIb_j/pub?output=csv\", header = TRUE)\n\nfake_insects &lt;- read.csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0snHMdsxzzzkxt_JVRFooJDB60lGSJQlrjUU29tGYOhIpqvx_pzja3Eqr9l5b4f76yMFvkiGzuK1Z/pub?output=csv\")\n\nThese two files will be used throughout the next chapters. The first is some made-up data of some mammal captures at 6 sites across forest and savanna habitats (with mass and parasite info), and the second is made up sweep-netting data from the same sites.\n\n\n10.1.4 Other File Types\nUnder construction…."
  },
  {
    "objectID": "r_import.html#checking-data",
    "href": "r_import.html#checking-data",
    "title": "10  Importing Data",
    "section": "10.2 Checking Data",
    "text": "10.2 Checking Data\nNow that you have data, you will want to look at it!\n\n10.2.1 The Whole Table\nYou can look at a whole data frame by clicking on its name in the “Environment” pane in RStudio (upper right), or with the View() function:\n\nView(fake_mammals)\n\nYou can also just look at parts:\n\n## check top 6 rows\nhead(fake_mammals)\n\n  site site_type            species mass_g tick_count helminth_mass_mg\n1    a    forest White-footed mouse     20          0              512\n2    a    forest White-footed mouse     24         10              365\n3    a    forest White-footed mouse     23          2                0\n4    a    forest White-footed mouse     19          0              608\n5    a    forest White-footed mouse     25         12              109\n6    a    forest         Deer mouse     22          3              456\n\n## check bottom 6 rows\ntail(fake_mammals)\n\n   site site_type            species mass_g tick_count helminth_mass_mg\n43    f   savanna White-footed mouse     21          0              408\n44    f   savanna White-footed mouse     25          1              197\n45    f   savanna White-footed mouse     24          0              152\n46    f   savanna         Deer mouse     20          0              508\n47    f   savanna         Deer mouse     22          2              496\n48    f   savanna        Meadow vole     23         NA               56\n\n\nYou can also take a look at the structure of the data with str(), which will tell you how many rows (observations) and how many columns (variables), as well as the type of each column.\n\n## check structure\nstr(fake_mammals)\n\n'data.frame':   48 obs. of  6 variables:\n $ site            : chr  \"a\" \"a\" \"a\" \"a\" ...\n $ site_type       : chr  \"forest\" \"forest\" \"forest\" \"forest\" ...\n $ species         : chr  \"White-footed mouse\" \"White-footed mouse\" \"White-footed mouse\" \"White-footed mouse\" ...\n $ mass_g          : int  20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int  0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int  512 365 0 608 109 456 521 432 20 129 ...\n\n\n\n\n10.2.2 Individual Columns\nYou can also take a look at individual columns with the $ operator, and get quick summaries with summary():\n\n## summarize mammal masses\nsummary(fake_mammals$mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  17.00   20.75   23.00   22.60   25.00   28.00 \n\n## summarize helminth masses\nsummary(fake_mammals$helminth_mass_mg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0   119.5   341.5   306.5   501.2   713.0 \n\n\nNote that for the second summary (helminth mass), it tells you how many NA’s, or missing values, there are.\n\n\n10.2.3 Factors: The Pseudo Data Type\nUnless you specify, csv reading functions will assume the data type of each column in a data sheet (numeric, character, etc.). Any categorical variable will be considered a character type generally. But the way character types are stored in computer memory does not lend itself well to statistical analysis. To remedy this, R has a special data type called the factor for categorical data. A factor is made up of two parts - the levels, which are stored to the computer as integers, and the labels, which are character strings that we can read as the category names.\nIn our mammal data, we probably want mammal species, site type, and site all to be factors. We can convert them with the as.factor function!\n\n## convert species to factor\nfake_mammals$species &lt;- as.factor(fake_mammals$species)\n## convert site type to factor\nfake_mammals$site_type &lt;- as.factor(fake_mammals$site_type)\n## convert site to a factor\nfake_mammals$site &lt;- as.factor(fake_mammals$site)\n\n## check it out!\nsummary(fake_mammals$species)\n\n        Deer mouse        Meadow vole White-footed mouse \n                15                  9                 24 \n\n## also do for insects\nfake_insects$site &lt;- as.factor(fake_insects$site)\nfake_insects$site_type &lt;- as.factor(fake_insects$site_type)\n\nThere are also similar functions for converting data types to numeric (as.numeric()) and character (as.character())."
  },
  {
    "objectID": "r_wrangle.html#adding-columns",
    "href": "r_wrangle.html#adding-columns",
    "title": "11  Wrangling Data",
    "section": "11.1 Adding Columns",
    "text": "11.1 Adding Columns\nOne simple thing you may want to do is add columns to your data, which may be calculations from existing columns.\n\n11.1.1 Base\nIn base R, we have already kinda done this. You can assign something to a new column with the $ and &lt;- operators.\nFor the insect data, let’s say we wanted to calculate average temperature at a given site based on the recorded high (temp_hi) and low (temp_low):\n\n## calculate mean temp\nfake_insects$temp_mean &lt;- (fake_insects$temp_hi + fake_insects$temp_lo)/2\n\n## check it out\nfake_insects$temp_mean\n\n[1] 21.25 23.25 19.25 26.75 24.00 28.00\n\n\n\n\n11.1.2 tidyverse\nIn the tidyverse, adding new columns is done with the mutate function:\n\n## load tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## mutate a new column\nfake_insects &lt;- mutate(.data = fake_insects, ## specify data\n                       temp_mean_mutated = (temp_hi + temp_lo)/2) ## calculate new column\n\n## this column should be the same for all six rows (a TRUE should be returned for each)\nfake_insects$temp_mean == fake_insects$temp_mean_mutated\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nAs you can see, using mutate mean you have to write the name of data frame fewer times."
  },
  {
    "objectID": "r_wrangle.html#pivoting-reshaping",
    "href": "r_wrangle.html#pivoting-reshaping",
    "title": "11  Wrangling Data",
    "section": "11.2 Pivoting / Reshaping",
    "text": "11.2 Pivoting / Reshaping\nYou also may need to transform your data between the wide and long formats (recall the data management slides, which will be added to this book eventually). I find that the pivot functions from tidyr in the tidyverse are easier to use, so we will go over those. But by all means, if you prefer base, go for it! I’m just less familiar with using base R for this.\n\n11.2.1 Wide to Long\nThe insect data is partly in wide format: we have a column for each order of insect, where the count is implicitly the values in the cells. IT will be easier to work with if we make one “count” column and one “order” column. We can do this with the pivot_longer function:\n\n## lengthen the order count data\nlong_insects &lt;- pivot_longer(data = fake_insects, \n                             cols = c(hymenoptera, lepidoptera, coleoptera, diptera, \n                                      odonata, hemiptera, orthoptera, ephemeroptera, \n                                      tricoptera, plecoptera),\n                             names_to = \"order\",\n                             values_to = \"count\"\n                             )\n\n## check it out\nhead(long_insects)\n\n# A tibble: 6 × 8\n  site  site_type temp_hi temp_lo temp_mean temp_mean_mutated order       count\n  &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n1 a     forest       23.5      19      21.2              21.2 hymenoptera     9\n2 a     forest       23.5      19      21.2              21.2 lepidoptera     3\n3 a     forest       23.5      19      21.2              21.2 coleoptera     16\n4 a     forest       23.5      19      21.2              21.2 diptera        29\n5 a     forest       23.5      19      21.2              21.2 odonata         4\n6 a     forest       23.5      19      21.2              21.2 hemiptera      10\n\n\nNow we have multiple rows for each site, one for each order! You may not believe me, but this will make things easier down the line.\nFor reference, the cols argument specifies which columns you want to pivot. The names_to argument names the column that will contain the pivoted column names, and the values_to argument names the column that the cell values will be placed.\n\n\n11.2.2 Long to Wide\nYou’ll occasionally want to turn long to wide as well. pivot_wider works for this\n\n## widen our long data\nwide_insects &lt;- pivot_wider(data = long_insects,\n                            names_from = \"order\",\n                            values_from = \"count\")\n\nThis is the inverse of what we just did - we made a column for each value in the column given to the names_from argument, the values of which are pulled from the column given to the values_from argument.\n\n\n11.2.3 Base\nYou can pivot data in base R with reshape, but this section is under construction…"
  },
  {
    "objectID": "r_wrangle.html#string-manipulation",
    "href": "r_wrangle.html#string-manipulation",
    "title": "11  Wrangling Data",
    "section": "11.3 String Manipulation",
    "text": "11.3 String Manipulation\nUnder construction….\n\n11.3.1 Base\n\n\n11.3.2 tidyverse"
  },
  {
    "objectID": "r_summarize.html#describing-with-summaries",
    "href": "r_summarize.html#describing-with-summaries",
    "title": "12  Summarizing Data",
    "section": "12.1 Describing With Summaries",
    "text": "12.1 Describing With Summaries\nYou’ll often want to give simple, illustrative information about the data you collected. The tidyverse is great for this!\n\n12.1.1 tidyverse\nThe package dplyr in the tidyverse has two wonderful functions: group_by() and summarize(). You can also use the British spelling, “summarise()”, but I use a z because it’s what the founders would have wanted.\nBefore we use these however, we need to introduce a very useful operator, the pipe: %&gt;%. This operator directs data into the first argument of a function, which allows you to chain functions together efficiently. Let’s try an example with the filter and select subsetting functions (see Section 9.2.3):\n\n## grab only the forest sites from the insect data\nforest_sites &lt;- fake_insects %&gt;% ## take fake_insects and pipe it into filter...\n  filter(site_type == \"forest\") %&gt;% ## filter only forest rows, pipe into select\n  select(site) ## select only the site column\n## the whole pipe chain is assigned to \"forest_sites\"\n\nforest_sites\n\n  site\n1    a\n2    b\n3    c\n\n## this is the same as\nforest_rows &lt;- filter(fake_insects, site_type == \"forest\")\nforest_sites &lt;- select(forest_rows, site)\n\nNote: base R also has a pipe, |&gt;. It’s newer and mostly the same as %&gt;%, so I just haven’t transitioned.\nNow, let’s try with group_by() and summarize()! Let’s say you wanted the total number of insects caught at each site (be sure to have pivoted your insect data in the last chapter!):\n\n## summarize total insect catch\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site) %&gt;% ## group observations\n  summarize(total_insects = sum(count)) ## sum all insects\n\ninsect_counts\n\n# A tibble: 6 × 2\n  site  total_insects\n  &lt;fct&gt;         &lt;int&gt;\n1 a                84\n2 b                87\n3 c               136\n4 d                60\n5 e                61\n6 f                51\n\n\nAs you can see, the summarize function works a bit like the mutate function: you\nNote that group_by doesn’t visibly change your data, but it changes some attributes that the computer can see when it runs the summarize function. If you forgot which sites are in which type of habitat, you could also include that variable in the group_by arguments (since it doesn’t subdivide the sites, it won’t change the calculation).\n\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site, site_type) %&gt;%\n  summarize(total_insects = sum(count))\n\nYou can also calculate means and variances! You can use the mean, var, and sd functions. Let’s try for each order across all sites:\n\norder_summary &lt;- long_insects %&gt;%\n  group_by(order) %&gt;%\n  summarize(count_mean = mean(count), ## you can do multiple sumaries at once\n            count_var = var(count),\n            count_sd = sd(count))\n\nhead(order_summary)\n\n# A tibble: 6 × 4\n  order         count_mean count_var count_sd\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 coleoptera         15.8      43.4      6.59\n2 diptera            17       163.      12.8 \n3 ephemeroptera       5.33    171.      13.1 \n4 hemiptera           6.17     42.6      6.52\n5 hymenoptera         8.5       9.1      3.02\n6 lepidoptera         5.67      5.87     2.42\n\n\nYou could also do this separately by site type:\n\norders_by_habitat &lt;- long_insects %&gt;%\n  group_by(site_type, order) %&gt;%\n  summarize(count_mean = mean(count), ## you can do multiple sumaries at once\n            count_var = var(count),\n            count_sd = sd(count))\n\n`summarise()` has grouped output by 'site_type'. You can override using the\n`.groups` argument.\n\nhead(orders_by_habitat)\n\n# A tibble: 6 × 5\n# Groups:   site_type [1]\n  site_type order         count_mean count_var count_sd\n  &lt;fct&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 forest    coleoptera         20.7      17.3      4.16\n2 forest    diptera            28.3       9.33     3.06\n3 forest    ephemeroptera      10.7     341.      18.5 \n4 forest    hemiptera          11.7      14.3      3.79\n5 forest    hymenoptera        10.3       2.33     1.53\n6 forest    lepidoptera         4.33      2.33     1.53\n\n\nFinally, you can save summarized output with write.csv() or write_csv():\n\n## save summary to your working/project directory\n## first argument is data, second argument is filename\nwrite.csv(orders_by_habitat, \"summary of orders\")\n\n\n\n12.1.2 Base\naggregate function (under construction…)\n\n\n12.1.3 Making Nice Tables\nthe gt package is good for this (under construction….)"
  },
  {
    "objectID": "r_summarize.html#community-ecology",
    "href": "r_summarize.html#community-ecology",
    "title": "12  Summarizing Data",
    "section": "12.2 Community Ecology",
    "text": "12.2 Community Ecology\nAverages and variances are all well and good but what about ecological measures?\n\n12.2.1 Richness\nYou may be interested in how many insect orders are represented in each site.\nLet’s do it in a pipe chain!\n\n## order presence\norder_richness_site &lt;- long_insects %&gt;% \n  mutate(presence = as.numeric(count &gt; 0)) %&gt;% ## create binary presence column\n  group_by(site, site_type) %&gt;%\n  summarize(order_richness = sum(presence))\n\n`summarise()` has grouped output by 'site'. You can override using the\n`.groups` argument.\n\norder_richness_site\n\n# A tibble: 6 × 3\n# Groups:   site [6]\n  site  site_type order_richness\n  &lt;fct&gt; &lt;fct&gt;              &lt;dbl&gt;\n1 a     forest                 8\n2 b     forest                 7\n3 c     forest                 9\n4 d     savanna                6\n5 e     savanna                7\n6 f     savanna                6\n\n\nI caclulated the presence column by checking if each value is positive (&gt; 0), which returns a logical TRUE or FALSE, and then if you convert a logical variable to a numeric variable, TRUEs become 1s and FALSEs become 0s. Nifty!\n\n\n12.2.2 Diversity\nvegan package (under construction…)"
  },
  {
    "objectID": "r_summarize.html#related-topic-joining-data",
    "href": "r_summarize.html#related-topic-joining-data",
    "title": "12  Summarizing Data",
    "section": "12.3 Related Topic: Joining Data",
    "text": "12.3 Related Topic: Joining Data\nSometimes with summaries, you will want to connect them to other pieces of data. Here we have some insect counts by site, and some mammal data by site. Let’s connect them! We can use the “merge” function from base R or the “join” functions from the tidyverse.\nWith merge:\n\n## grab only the site and total columns from insect_counts\n## this prevent doubling the site_type column\nmerged_data &lt;- merge(insect_counts[,c(\"site\", \"total_insects\")], fake_mammals, by = \"site\")\n\n## look at the new column in your data\nstr(merged_data)\n\n'data.frame':   48 obs. of  7 variables:\n $ site            : Factor w/ 6 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ total_insects   : int  84 84 84 84 84 84 84 84 84 84 ...\n $ site_type       : Factor w/ 2 levels \"forest\",\"savanna\": 1 1 1 1 1 1 1 1 1 1 ...\n $ species         : Factor w/ 3 levels \"Deer mouse\",\"Meadow vole\",..: 3 3 3 3 3 1 1 1 2 2 ...\n $ mass_g          : int  20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int  0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int  512 365 0 608 109 456 521 432 20 129 ...\n\n\nWith join:\n\n## there are different join functions for different contexts\n## left_join keeps every row from the first data frame and adds any matching rows from the\n## second. it works in most cases\n## inner_join and full_join can also be useful\njoined_data &lt;- insect_counts %&gt;%\n  select(total_insects, site) %&gt;%\n  left_join(fake_mammals, by = \"site\")\n\n## look at it\nstr(joined_data)\n\ntibble [48 × 7] (S3: tbl_df/tbl/data.frame)\n $ total_insects   : int [1:48] 84 84 84 84 84 84 84 84 84 84 ...\n $ site            : Factor w/ 6 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ site_type       : Factor w/ 2 levels \"forest\",\"savanna\": 1 1 1 1 1 1 1 1 1 1 ...\n $ species         : Factor w/ 3 levels \"Deer mouse\",\"Meadow vole\",..: 3 3 3 3 3 1 1 1 2 2 ...\n $ mass_g          : int [1:48] 20 24 23 19 25 22 22 21 23 20 ...\n $ tick_count      : int [1:48] 0 10 2 0 12 3 2 0 NA NA ...\n $ helminth_mass_mg: int [1:48] 512 365 0 608 109 456 521 432 20 129 ..."
  },
  {
    "objectID": "r_analyze.html#making-comparisons",
    "href": "r_analyze.html#making-comparisons",
    "title": "13  Analyzing Data",
    "section": "13.1 Making Comparisons",
    "text": "13.1 Making Comparisons\nFirst off, let’s just do some simple comparisons.\n\n13.1.1 t-tests\nLet’s say we want to compare two groups, like the number of insects caught in forests and savannas. We already created a summary of this in the last chapter:\n\n## sum insects by site\ninsect_counts &lt;- long_insects %&gt;%\n  group_by(site, site_type) %&gt;%\n  summarize(total_insects = sum(count))\n\n`summarise()` has grouped output by 'site'. You can override using the\n`.groups` argument.\n\ninsect_counts\n\n# A tibble: 6 × 3\n# Groups:   site [6]\n  site  site_type total_insects\n  &lt;fct&gt; &lt;fct&gt;             &lt;int&gt;\n1 a     forest               84\n2 b     forest               87\n3 c     forest              136\n4 d     savanna              60\n5 e     savanna              61\n6 f     savanna              51\n\n\nIt seems that there may be a difference! So let’s run a t-test to test for a difference in means between two groups (see Section 4.1.2.1). Most stats functions in R can use the formula operator, ~. This allows us to connect our dependent variable (insect count in this case) as a function of our independent variable (site habitat type): total_insects ~ site_type.\n\n## run the t test\nhabitat_comparison &lt;- t.test(formula = insect_counts$total_insects ~ insect_counts$site_type)\n\n## check the output\nhabitat_comparison\n\n\n    Welch Two Sample t-test\n\ndata:  insect_counts$total_insects by insect_counts$site_type\nt = 2.6235, df = 2.1422, p-value = 0.1116\nalternative hypothesis: true difference in means between group forest and group savanna is not equal to 0\n95 percent confidence interval:\n -24.30218 114.30218\nsample estimates:\n mean in group forest mean in group savanna \n            102.33333              57.33333 \n\n\nIf we look at the ouput, it look like the forest mean was 102.333, and the savanna mean was 57.333, for a mean difference or effect size of 45. The p-value, or how strong the evidence for a relationship is, is 0.1116. This is higher than the traditional threshold for significance, likely because we have a very small sample size (6 total).\nNote: if you check the help for the t.test function (run ?t.test), you can find arguments for paired t-tests (paired) and unequal variances among groups (var.equal).\n\n\n13.1.2 ANOVA\nWhat if we have more than two categories, and we want to see if any two categories have different means? Let us return to the mammal data and compare the mass of helminths (parasitic worms) in different mammal species. We can run an analysis of variance (see Section 4.1.2.2).\n\n## run the anova\nhelminth_comparison &lt;- aov(fake_mammals$helminth_mass_mg ~ fake_mammals$species)\n\n## check the output (now with the summary function)\nsummary(helminth_comparison)\n\n                     Df  Sum Sq Mean Sq F value  Pr(&gt;F)    \nfake_mammals$species  2  719721  359861   10.59 0.00017 ***\nResiduals            45 1529035   33979                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHey, that’s a small p-value (0.00017)! That means we have strong evidence that there is at least one difference among the pairs of species, either between white-footed mice and deer mice, white-footed mice, and meadow voles, or deer mice and meadow voles. We can use a Tukey test to find out more:\n\n## run tukey on the anova output\nhelminth_tukey &lt;- TukeyHSD(helminth_comparison)\n\n## check it out\nhelminth_tukey\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fake_mammals$helminth_mass_mg ~ fake_mammals$species)\n\n$`fake_mammals$species`\n                                    diff        lwr        upr     p adj\nMeadow vole-Deer mouse         -356.2222 -544.58911 -167.85534 0.0001061\nWhite-footed mouse-Deer mouse  -111.3333 -258.37719   35.71052 0.1698916\nWhite-footed mouse-Meadow vole  244.8889   70.26811  419.50966 0.0039996\n\n\nAt the bottom here we can see the pairwise comparisons. The two mouse species differ in helminth mass by ~111mg, but the difference is not significant. Meadow voles have a significantly different mean helminth mass from both mouse species. I guess the in the data I made up, voles have less helminth mass than mice."
  },
  {
    "objectID": "r_analyze.html#assessing-relationships",
    "href": "r_analyze.html#assessing-relationships",
    "title": "13  Analyzing Data",
    "section": "13.2 Assessing Relationships",
    "text": "13.2 Assessing Relationships\nBut what if you’re not dealing with categorical comparisons? Then we can check for numerical associations.\n\n13.2.1 Correlation\nWe can look for simple associations without cause and effect with correlations (see Section 4.1.3.1). Mice seem to have high helminth loads, so let’s check for a correlation between their body mass and helminth mass:\n\n## create a subset of only mouse data\n## I use the %in% operator to specify that species should be found in a specified vector\n## AKA, it could be white-footed mouse OR deer mouse\nmouse_data &lt;- filter(fake_mammals, species %in% c(\"White-footed mouse\", \"Deer mouse\"))\n\n## run correlation with two variables (no formula here)\ncor.test(mouse_data$helminth_mass_mg, mouse_data$mass_g)\n\n\n    Pearson's product-moment correlation\n\ndata:  mouse_data$helminth_mass_mg and mouse_data$mass_g\nt = -10.941, df = 37, p-value = 3.741e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9323983 -0.7711347\nsample estimates:\n       cor \n-0.8740017 \n\n\nHere we get an effect size of -0.874 (correlation coefficient), and a p-value of 3.741e-13, which means 3.741 x 10-13, or &lt;&lt;&lt;0.001. This means there is a strong negative relationship observed between mouse mass and helminth mass, and we have very strong evidence for it.\n\n\n13.2.2 Linear Regression\nIf we want to infer cause and effect we can use linear regression (see Section 4.1.3.2). Let’s say we want to know if the number of insects at a site is predictive of mammal mass at a site. First let’s join the two data frames like wed did in the last chapter:\n\n## join our data\nmammals_insects &lt;- insect_counts %&gt;%\n  select(total_insects, site) %&gt;%\n  left_join(fake_mammals, by = \"site\")\n\n## regress mammal mass on total insects with lm function\n## this time I'm specifying the data frame with the data argument\n## then I don't have to write it twice\nmass_model &lt;- lm(mass_g ~ total_insects, data = mammals_insects)\n\n## look at the ouput with summary again\nsummary(mass_model)\n\n\nCall:\nlm(formula = mass_g ~ total_insects, data = mammals_insects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6018 -1.5936  0.0221  2.0487  4.3982 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.15860    1.09484  18.412   &lt;2e-16 ***\ntotal_insects  0.02808    0.01184   2.372   0.0219 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.549 on 46 degrees of freedom\nMultiple R-squared:  0.109, Adjusted R-squared:  0.08959 \nF-statistic: 5.625 on 1 and 46 DF,  p-value: 0.02195\n\n\nIf we look at the coefficient table, we can see that the total insect term has an estimate of 0.028, which is our effect size. For every added insect to a plot, the expected average mass of the mammal community goes up by 0.028g. Connected to that effect size is a p-value of 0.0219, which means we have strong evidence for the relationship.\n\n\n13.2.3 Binomial Regression\nIf your response variable is binary (presence absence), you can use a binomial regression with the glm() function. Let’s test if mammal mass effects the probability of having ticks attached (tick_count).\n\n## first make a presence absence variable for ticks\nfake_mammals$tick_presence &lt;- as.numeric(fake_mammals$tick_count &gt; 0)\n\n## now do the regression, with the binomial \"family\"\ntick_pres_model &lt;- glm(tick_presence ~ mass_g, data = fake_mammals, family = \"binomial\")\n\n## check it\nsummary(tick_pres_model)\n\n\nCall:\nglm(formula = tick_presence ~ mass_g, family = \"binomial\", data = fake_mammals)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7550  -0.4805   0.1027   0.4404   1.4731  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -22.3584     6.7603  -3.307 0.000942 ***\nmass_g        0.9857     0.2950   3.341 0.000835 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 54.040  on 38  degrees of freedom\nResidual deviance: 28.181  on 37  degrees of freedom\n  (9 observations deleted due to missingness)\nAIC: 32.181\n\nNumber of Fisher Scoring iterations: 5\n\n\nIf we look at this like we looked at the linear regression, the mass_g term has a very small p-value meaning strong evidence for a relationship. It also has an effect size of 0.9857, meaning that the chance of having a tick increases with body mass. However, the units are in log odds, which are hard to interpret. The reason for this is some stats theory that is beyond the scope of this book.\n\n\n13.2.4 Poisson / Negative Binomial Regression\nunder construction…."
  },
  {
    "objectID": "r_analyze.html#multivariate-analysis",
    "href": "r_analyze.html#multivariate-analysis",
    "title": "13  Analyzing Data",
    "section": "13.3 Multivariate Analysis",
    "text": "13.3 Multivariate Analysis\nYou can of course use multiple explanatory variables in your analyses. For example, when we regressed mammal mass on insect count, we ignored mammal species. We could include it like so:\n\n## multiple regression\nmulti_mod &lt;- lm(mass_g ~ total_insects + species, data = mammals_insects)\n\nsummary(multi_mod)\n\n\nCall:\nlm(formula = mass_g ~ total_insects + species, data = mammals_insects)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1418 -1.1856  0.1335  1.8338  5.1527 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               19.10561    1.36855  13.960  &lt; 2e-16 ***\ntotal_insects              0.03490    0.01272   2.744  0.00875 ** \nspeciesMeadow vole        -0.29446    1.08017  -0.273  0.78644    \nspeciesWhite-footed mouse  1.02943    0.86932   1.184  0.24270    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.545 on 44 degrees of freedom\nMultiple R-squared:  0.1505,    Adjusted R-squared:  0.09254 \nF-statistic: 2.598 on 3 and 44 DF,  p-value: 0.06421\n\n\nNow we have multiple terms, and since species is categorical, the effect sizes and p-values are based on comparisons to a reference level (deer mouse in this case because it is first alphabetically).\nWe can look at the overall significance of species by running an ANOVA with aov, and summarizing the ouptut:\n\n## we can use the model object in our aov function to save time, it will take the formula\nsummary(aov(multi_mod))\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntotal_insects  1  36.55   36.55   5.643 0.0219 *\nspecies        2  13.93    6.96   1.075 0.3501  \nResiduals     44 285.00    6.48                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooks like there is not much evidence of an effect of species.\nYou may be confused a bit by this code, but essentially t.tests, ANOVAs, and regressions are all “linear models”, and we are specifying them and looking at them differently with different R functions. T learn more, I recommend taking stats classes!\nFinally, you may be wondering which variable to include in your analyses. Model selection is another thing you would learn in stats, but the tl;dr could be: What is your question? Use those variables."
  },
  {
    "objectID": "r_visualize.html#general-notes-on-data-visualization",
    "href": "r_visualize.html#general-notes-on-data-visualization",
    "title": "14  Visualizing Data",
    "section": "14.1 General Notes on Data Visualization",
    "text": "14.1 General Notes on Data Visualization\nThere are a few things to keep in mind in general when creating figures, even outside of R:\nUsually, figures should stand alone. This means that your figure can speak for itself, even without a caption. This means that axes and legends are clearly labelled, and trends are emphasized. It can also be helpful to annotate statistical output onto plots themselves.\nWhen you can, show your actual data, instead of summary stats. Generally, when it’s not too messy, seeing all the data points is more informative to the audience. For example, you could plot a comparison of means with a point for each mean, but you could show more if you plot every point behind those means.\nIt is advisable to shoot for a low ink to info ratio. Basically, you want to make your figures as uncluttered as they can be with extra print. Gray backgrounds, irrelevant fill colors, etc. should be avoided.\nFinally, remember accessibility. Make color schemes appropriate for color-blindness, and make text large."
  },
  {
    "objectID": "r_visualize.html#the-tidyverses-ggplot2",
    "href": "r_visualize.html#the-tidyverses-ggplot2",
    "title": "14  Visualizing Data",
    "section": "14.2 The tidyverse’s ggplot2",
    "text": "14.2 The tidyverse’s ggplot2\nWhen it comes to visualizing things in R, there are many methods. You can use the base R functions for plotting (plot, hist, lines, etc.), but I’m not super adept with them. Instead I’ll be walking you through using ggplot2, a package in the tidyverse family that is incredibly popular for data visualization. There is a special syntax that may take some getting used to though.\nEssentially, you create a ggplot “object” (which is another special type of list with unique attributes), and then you pipe it through a series of ggplot functions to add components, themes, labels, etc. However, ggplot2 is older than the %&gt;% pipe we have used, so it uses an old and deprecated pipe operator: +. R automatically knows to interpret + differently with ggplot objects and functions.\nHere is an example of code creating a ggplot figure:\n\n## first create the ggplot object\n## you need to specify your data in the data argument\n## then there is a special set of arguments called aesthetic arguments\n## (bound by the aes() sub-function)\n## these specify what variables will inform aesthetics of your figure\n## (e.g., axes, color, fills, sizes, etc.)\nggplot(data = your_data, aes(x = variable1, y = variable2, color = variable3)) +\n  geom_point(size = 2) + ## then you add geometry, this \"geom\" is for a scatterplot\n  labs(x = \"Variable 1\") + ## then you can add other things like labels\n  scale_color_manual(values = c(\"red\", \"blue\")) + ## or specify scales\n  theme(axis.text = element_text(size = 12)) ## finally you can modify parts of the theme, like fonts\n\nIt may seem complicated at first, but if you start small and work yourself up, you’ll be chaining together code to draw beautiful figures in no time!"
  },
  {
    "objectID": "r_visualize.html#figure-types",
    "href": "r_visualize.html#figure-types",
    "title": "14  Visualizing Data",
    "section": "14.3 Figure Types",
    "text": "14.3 Figure Types\nNow we’ll go over how to make some common figure types, based on your analyses.\n\n14.3.1 One Variable: Continuous\nIf you want to show the distribution of a single variable, you could use a histogram or a density plot.\nFor demonstration, let’s make a plots of white-footed mouse masses.\n\n## create a data frame of only white-footed mice\nwf_mice &lt;- filter(fake_mammals, species == \"White-footed mouse\")\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThere, a simple histogram. Now let’s play with how it looks:\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  ## give a wider binwidth to the histogram, and make it grey bars with black outlines\n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") + \n  labs(x = \"Mass (g)\", y = \"Count\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\nCool! We could also look at this as a density plot! This will give more of a smooth line\n\n## make a ggplot, use wf_mice data, and specify mass as the x variable\nggplot(data = wf_mice, aes(x = mass_g)) +\n  geom_density() + ## create density plot\n  labs(x = \"Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\n\n\n14.3.2 One Variable: Categorical\nIf you want to show how many observations are in each category, you can use a bar plot.\nIn this demo, let’s make a bar plot of how many of each mammal species were caught.\n\n## specify x as species\nggplot(data = fake_mammals, aes(x = species)) +\n  geom_bar() ## make bar plot\n\n\n\n\nThe geom_bar function will count up all the observations of each species level to inform its bars. Thus, it is assuming you are giving it long data. Another closely related function is geom_col, which just makes a bar as tall as a number value in the data. For example, let’s make a bar plot of how many insect were caught at each site.\n\n## need to specify two variables this time, one for the category, one for the count value\nggplot(data = insect_counts, aes(x = site, y = total_insects)) +\n  geom_col()\n\n\n\n\nAs you can see, your data format will determine whether you should use geom_col or geom_bar. Note: bar plots are generally only best-suited for counts among categories, when you’re dealing with measured variables, there are better options below.\n\n\n14.3.3 Two Variables: Both Continuous\nIf you are showing the relationship between two continuous variables, scatterplots with or without lines are usually the best way to go.\nLet’s try it out with the mammal data on body mass and helminth mass in mice:\n\n## filter for mouse data\nmouse_data &lt;- filter(fake_mammals, species %in% c(\"White-footed mouse\", \"Deer mouse\"))\n\n## create ggplot with your two continuous variables as x and y\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg)) +\n  geom_point() ## create scatterplot\n\n\n\n## with trendline\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg)) +\n  geom_point() +  ## create scatterplot\n  ## create a trendline; method = \"lm\" makes it a straight line, se specifys whether there are error regions shaded\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n14.3.4 Two Variables: One Continuous, One Categorical\nBelieve it or not, when one of your variables is categorical, a scatterplot is still appropriate. Why not a bar plot? Because scatterplots show all of your data!\nLet’s demonstrate with the mammal data by comparing helminth mass among species.\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(width = 0.1, height = 0) ## create points that are \"jittered\" a bit along the x axis\n\n\n\n\nIn this plot, we use geom_jitter to make the point spread a bit around each categorical X value so that you can see them better (but we specify height = 0 so as not to mess with the mass information). Instead of a mean helminth mass given by a bar plot, we can see the spread of each set of datapoints, including outliers or lack thereof. Still it’s often nice to add some structure to these plots, which can be geom_boxplot or geom_violin (among others). Here is an example:\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(width = 0.1, height = 0) + ## create points that are \"jittered\" a bit along the x axis\n  geom_boxplot(alpha = 0.2) ## create boxplot at 20% transparency with alpha\n\n\n\n\nWe could also make this plot even clearer by adding color:\n\n## create ggplot with your two variables as x and y\nggplot(data = fake_mammals, aes(x = species, y = helminth_mass_mg)) +\n  geom_jitter(aes(color = species), width = 0.1, height = 0) + ## you can put aes() inside geoms\n  geom_boxplot(aes(fill = species), alpha = 0.2) +\n  labs(x = \"Species\", y = \"Helminth Mass (mg)\") +\n  theme_bw() +\n  theme(legend.position = \"none\") ## legend is redundant here, so we can hide it\n\n\n\n\n\n\n14.3.5 Non-Axis Variables\nYou can also use other aesthetics to represent variables in your data. For example, you could use color to show the density plots of mammal masses among species. And you can modify the colors with scale functions:\n\n## make a ggplot, use wf_mice data, specify mass as the x variable and species as color\nggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density() + ## create density plot\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) + ## set my own colors\n  labs(x = \"Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\nSimilarly, you can add a third variable to a two variable figure. Take the helminth mass by mammal body mass figure from above:\n\nggplot(data = mouse_data, aes(x = mass_g, y = helminth_mass_mg, color = species)) +\n  geom_point() +  ## create scatterplot\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nColor isn’t the only way to show variables outside of axes, you can also use point shape, size, linetype, etc. In addition, you can split data among plot panels or “facets”, with facet_wrap() or facet_grid().\nLet’s demonstrate with the long insect data, showing the insect communities for each site:\n\n## specify order as y variable to show labels better\nggplot(data = long_insects, aes(y = order, x = count, fill = site_type)) +\n  geom_col() +\n  facet_wrap(vars(site), nrow = 2) + ## specify site variable, two rows to separate habitats\n  theme_bw()\n\n\n\n\n\n\n14.3.6 Colorblind Safe Colors\nggplot2 has colorblind-safe color schemes available from the tidyverse-related package viridis.\nFore example:\n\n## make a ggplot, use wf_mice data, specify mass as the x variable and species as color\nggplot(data = fake_mammals, aes(x = mass_g, color = species)) +\n  geom_density(linewidth = 1) + ## create density plot, wider lines\n  scale_color_viridis_d() + ## set viridis discrete colors\n  labs(x = \"Mass (g)\", y = \"Frequency\") + ## nicer labels\n  theme_bw() ## my favorite simple theme\n\n\n\n\nSee the following link for more info:\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html"
  },
  {
    "objectID": "r_visualize.html#further-reading",
    "href": "r_visualize.html#further-reading",
    "title": "14  Visualizing Data",
    "section": "14.4 Further Reading",
    "text": "14.4 Further Reading\nWe have only scratched the surface of what ggplot2 can do! We barely discussed how to edit theme elements, nor did we spend much time on customizing scales.\nggplot2 has an excellent reference website which you can find here:\nhttps://ggplot2.tidyverse.org/reference/index.html\nWith it you can learn all the ins and outs!"
  },
  {
    "objectID": "appendix_a.html",
    "href": "appendix_a.html",
    "title": "15  Appendix A: More R Resources",
    "section": "",
    "text": "Here is a collection of links to other useful resources for learning R!\nEcology-themed tutorial:\nhttps://datacarpentry.org/R-ecology-lesson/index.html\nBasic / Base R Materials\nOfficial R manuals:\nhttps://cran.r-project.org/manuals.html\nCookbook for R (features lots of “recipes” for common tasks)\nhttp://www.cookbook-r.com/\nPrimers and Cheatsheets (“tidyverse”-based)\nRStudio primers\nhttps://rstudio.cloud/learn/primers\nTidyverse cheatsheets\nhttps://www.rstudio.com/resources/cheatsheets/\nFull Books and Courses\nHadley Wickham’s “R for Data Science”\nhttps://r4ds.had.co.nz/index.html\nHadley Wickham’s “Advanced R”\nhttps://adv-r.hadley.nz/\nBook for “ggplot2” package\nhttps://ggplot2-book.org/\nJenny Bryan’s STAT 545 course\nhttp://stat545.com/\nPackage Function Reference Sites\nggplot2 (data visualization)\nhttps://ggplot2.tidyverse.org/index.html\nsf (spatial analysis)\nhttps://r-spatial.github.io/sf/index.html\nMiscellaneous Resources\nOn Style:\nhttp://adv-r.had.co.nz/Style.html\nOn Reproducibility:\nhttps://reproducible-analysis-workshop.readthedocs.io/en/latest/\nhttps://swcarpentry.github.io/r-novice-gapminder/"
  }
]